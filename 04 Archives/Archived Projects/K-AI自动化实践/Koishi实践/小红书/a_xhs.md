# 【超详细保姆级】Coze采集小红书工作流教程 (Obsidian版)
> **一句话概括本教程：** 我们要像搭建乐高积木一样，利用一个叫 Coze 的免费工具，打造一个能自动“访问”指定小红书主页，并把上面所有笔记“复制”下来的机器人。
## 准备工作：开启自动化之旅的“钥匙”
> **一句话概括本部分：** 在正式开始前，我们需要准备好三样东西：Coze 账号、你的小红书“通行证”(Cookie)，以及一个你想“学习”的小红书主页链接。
### 1. 注册并进入 Coze 工作区
Coze 是一个能让你创建 AI 机器人的平台，而**工作流 (Workflow)** 就是指挥这个机器人具体该如何一步步干活的“流程图”。
- **操作：**
    1. 在浏览器中搜索“扣子 Coze”或直接访问官网。
    2. 完成注册和登录。
    3. 在左侧菜单栏找到你的“个人空间”，点击进入。在这里，你将创造你的第一个工作流。
### 2. ==理解并获取 Cookie (最重要的“通行证”)==
> [!NOTE] 什么是 Cookie？
> 
> 通俗比喻： Cookie 就像是你进入小红书这个“会员俱乐部”的临时通行证。当你登录小红书后，网站会发给你这张卡，后续你在网站内的所有操作，都带着这张卡，网站就知道是你本人在操作。
> 
> **为什么我们需要它：** 我们的 Coze 机器人需要==模拟成一个已经登录的你==，去访问小红书主页。所以，我们必须把这张“通行证”交给它，小红书的服务器才会认为这个访问是合法的，并返回给我们数据。
- **如何获取 Cookie：**
    1. 在你的电脑浏览器（推荐 Chrome 或 Edge）上登录你的小红书账号。
    2. 安装一个名为 `Cookie-Editor` 的浏览器插件。你可以在浏览器的插件商店里搜索到它。
    3. 安装后，在小红书页面上，点击浏览器右上角的 `Cookie-Editor` 插件图标。
    4. 选择“导出” (Export) -> “导出为 JSON” (Export as JSON)，然后点击“复制到剪贴板” (Copy to Clipboard)。
    > [!WARNING] Cookie 的注意事项
    > 
    > - **隐私安全：** Cookie 包含了你的登录信息，==绝对不要泄露给任何人！==
    >     
    - **时效性：** Cookie 会过期。如果你的工作流突然失效，通常是因为 Cookie 过期了，你需要重新获取并替换掉旧的。
### 3. 找到你的对标账号
这个很简单，就是在小红书上找到你想要分析和学习的那个博主，复制他/她主页的浏览器地址栏链接。例如：`https://www.xiaohongshu.com/user/profile/xxxxxxxxxx`。
## Part 1: 创建并配置工作流的“起点”
> **一句话概括本部分：** 我们来搭建工作流的起点，并告诉它，每次运行时我们需要提供哪些初始信息。
1. 在 Coze 的工作空间里，点击 **“创建工作流”**。
2. 给它起一个你能记住的名字，比如视频里提到的：“小红书对标账号主页采集同步飞书仿写”。
3. 你会看到一个默认的 `开始` 和 `结束` 节点。`开始` 节点就是我们整个流程的入口。
### 配置 `开始` 节点
我们需要在这里定义两个输入变量，方便我们后续灵活使用：
- **变量1：`homepage_url` (主页链接)**
    - 点击 `开始` 节点右侧的 `+` 号，添加一个输入。
    - **名称：** `homepage_url`
    - **类型：** 字符串 (String)
    - **作用：** 用来存放我们想要采集的那个小红书主页的链接。
- **变量2：`my_cookie` (我的通行证)**
    - 再次点击 `+` 号，添加第二个输入。
    - **名称：** `my_cookie`
    - **类型：** 字符串 (String)
    - **作用：** 用来存放我们刚刚获取的小红书 Cookie。把它设为变量的好处是，当 Cookie 过期时，我们只需要在运行时输入新的，而不用修改工作流内部。
**配置完成后，你的 `开始` 节点看起来应该是这样的：**
```
输入：
- homepage_url (String)
- my_cookie (String)
```
## Part 2: 搭建核心流程：两步获取所有数据
> **一句话概括本部分：** 我们将用两个关键的“积木”（插件），第一步从主页上拿到所有笔记的列表，第二步再根据列表一个个地获取笔记的详细内容。
### 步骤一：获取用户笔记列表 (拿到所有笔记的“门牌号”)
> [!TIP] 这一步的目标
> 
> 我们的目标不是直接获取笔记内容，而是先获取到该主页下==所有笔记的独立链接==。就像你想进一栋楼里所有的房间，你得先拿到所有房间的门牌号列表。
1. 在节点列表中，点击 `+` 号，搜索“小红书”。
2. 在插件列表中，找到并选择 **`XiaoHongShu.get_user_notes_by_homepage_url`** (获取用户笔记列表)。
3. 把它拖拽到画布上，并给它起个别名，例如：“获取主页的笔记列表”。
#### 连接节点 (让数据流动起来)
- **`cookie` 输入：** 从 `开始` 节点的 `my_cookie` 输出点，拉一根线连接到“获取主页的笔记列表”节点的 `cookie` 输入点。
- **`homepage_url` 输入：** 从 `开始` 节点的 `homepage_url` 输出点，拉一根线连接到“获取主页的笔记列表”节点的 `homepage_url` 输入点。
#### 测试一下！
现在我们可以点击右上角的“运行”来测试一下。
- **输入 `my_cookie`:** 粘贴你之前复制的整段 Cookie JSON 文本。
- **输入 `homepage_url`:** 粘贴你要采集的主页链接。
- **点击运行。**
运行成功后，你会看到这个节点输出了很多数据。展开 `notes` 字段，你会发现它是一个**列表 (Array)**，里面包含了多条笔记的摘要信息。视频中这里获取到了 `$13` 条。
**==最关键的信息是，每一条里面都有一个 `url` 字段，这就是单篇笔记的链接，也就是我们需要的“门牌号”！==**
**输出数据结构（简化示例）：**
```
{
  "notes": [
    {
      "note_id": "xxxx1",
      "title": "33岁小阿姨，我还有机会吗",
      "type": "normal",
      "url": "https://www.xiaohongshu.com/explore/xxxx1" // <-- 这是我们想要的！
    },
    {
      "note_id": "xxxx2",
      "title": "另一篇笔记标题",
      "type": "video",
      "url": "https://www.xiaohongshu.com/explore/xxxx2" // <-- 这也是我们想要的！
    }
    // ...更多笔记
  ]
}
```
### 步骤二：获取笔记详情 (拿着“门牌号”进屋抄录)
> [!TIP] 这一步的目标
> 
> 我们现在手握所有笔记的链接列表，接下来要做的就是==循环遍历==这个列表，用每个链接去获取对应笔记的标题、正文、点赞数、评论数等所有详细信息。
1. 再次点击 `+` 添加节点，搜索“小红书”，选择 **`XiaoHongShu.get_note_by_url`** (根据笔记链接获取笔记详情)。
2. 拖拽到画布上，起个别名，例如：“获取单篇笔记详情”。
#### 连接节点
- **`cookie` 输入：** 同样地，从 `开始` 节点的 `my_cookie` 拉线到这个新节点的 `cookie` 输入。
- **`url` 输入：** ==这是最关键的一步！== 从“获取主页的笔记列表”节点的 `notes` **输出点**拉线，连接到“获取单篇笔记详情”节点的 `url` **输入点**。
Coze 的智能之处：
当你这样连接时，Coze 会自动识别到你正在把一个列表 (多条笔记) 连接到一个只能处理单项 (单个链接) 的输入上。它会自动为你插入一个“循环”或“遍历”的逻辑。这意味着，它会自动对“获取主页的笔记列表”输出的每一项，都执行一次“获取单篇笔记详情”的操作。
#### 再次测试！
再次点击“运行”，使用相同的 Cookie 和主页链接。这次，你会看到“获取单篇笔记详情”这个节点会运行多次（你有多少篇笔记，它就运行多少次）。
每一次的输出，都是一篇笔记的完整信息。
**输出数据结构（简化示例）：**
```
{
  "note_detail": {
    "title": "33岁小阿姨，我还有机会吗",
    "desc": "33岁读4年，没有什么内容...", // <-- 笔记正文
    "type": "normal",
    "interact_info": {
      "liked_count": "2147", // <-- 点赞数，视频里的例子是 $2147$
      "comment_count": "300",
      "share_count": "100",
      "collected_count": "500"
    },
    "user": {
      "nickname": "博主昵称",
      "user_id": "yyyyy"
    }
    // ...海量其他详细信息
  }
}
```
## 总结与下一步
> **一句话概括：** 恭喜！你已经成功搭建了一个可以自动从任意小红书主页抓取所有笔记详细数据的机器人，现在我们手上有了“原材料”，下一步就是对它们进行“深加工”。
到目前为止，我们已经完成了视频中演示的所有核心抓取步骤。我们获取到的数据虽然完整，但还是“生肉”，结构很乱。
**接下来的任务（视频未详细展开，但属于完整流程）：**
1. **数据处理：** 将这些零散的 JSON 数据进行清洗和整理，提取出我们真正关心的字段（如：标题、正文、点赞、评论、收藏数）。这通常会用到`代码`节点。
2. **写入飞书/Excel：** 将整理好的数据，通过飞书多维表格的插件，一行行地写入到你的在线表格中，形成一个清晰、可分析的数据看板。
3. **智能仿写：** 利用大语言模型（LLM）节点，将获取到的爆款笔记喂给 AI，让它根据笔记的模式和主题，为你生成新的、高质量的仿写文案。
希望这份超级详细的教程能帮您彻底弄懂这个工作流的原理和操作！如果您对某一步还有疑问，随时可以提出来，我很乐意为您解答。