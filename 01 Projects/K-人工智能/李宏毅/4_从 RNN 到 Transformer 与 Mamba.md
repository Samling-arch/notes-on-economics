# 神经网络架构的演进：从 RNN 到 Transformer 与 Mamba
> 这份笔记的核心思想是：**“每一种神经网络架构的诞生，都是为了解决一个特定的问题。”** 就像我们为了不同的任务会使用不同的工具（用锤子钉钉子，用螺丝刀拧螺丝），在机器学习中，我们也会设计不同的网络架构来应对不同的挑战。
## 1. 为什么我们需要不同的网络架构？
> **一句话概括：** 不同的架构是针对不同问题（如图像识别、处理长句子）的“特攻型选手”，它们通过优化网络结构来提升效率和效果。
我们学习网络架构，不仅仅是学习一堆复杂的模型，更重要的是理解其背后的 **“设计理念”** (Reason for Existence)。每一个经典架构的出现，都是一次聪明的“妥协”与“进化”。
### 1.1 卷积神经网络 (CNN) - 为“看图”而生
> **一句话概括：** CNN 是一个“删减版”的全连接网络，它根据图像的特性，只保留了局部视野和共享参数，从而大大减少了训练难度。
- **设计初衷：** 解决传统全连接前馈网络 (Fully Connected Feedforward Network) 在处理图像时参数过多的问题。
- **核心思想：**
    1. **感受野 (Receptive Field):** 神经元不需要“看”整张图，只需要关注一小块局部区域。这就像我们看一幅巨大的画，会先聚焦于某个细节，而不是一眼就想看清所有内容。
    2. **参数共享 (Parameter Sharing):** 用于识别图片左上角“猫耳朵”的参数，同样可以用于识别右下角的“猫耳朵”。这个“识别器”（称为卷积核）在整张图上滑动，共享同一套参数。
- **带来的好处：**
    - **减少参数量：** 参数少了，模型就不那么容易 **过拟合 (Overfitting)**。过拟合就像一个学生只会死记硬背考卷上的原题，换个问法就不会了。参数减少意味着模型学习到的是更通用的知识，而不是死记硬背训练数据。
    - **训练更容易：** 用更少的数据就能训练出一个好模型。
> **小结：** CNN 的存在理由是 **“根据图像的局部性与平移不变性，来减少不必要的参数”**。
### 1.2 残差连接 (Residual Connection) - 为了“建得更深”
> **一句话概括：** 残差连接就像给神经网络修了一条“高速公路”，让信息可以直接跳过好几层，从而解决了网络太深难以训练的问题。
- **设计初衷：** 解决一个奇怪的现象——“网络越深，效果越差”。
- **现象观察：** 实验发现，一个 `$56$` 层的网络，无论是在测试集还是 **训练集** 上，表现都比 `$20$` 层的网络要差。
    - **重点：** 既然在训练集上效果也差，那就不是过拟合（参数过多）的问题，而是 **优化困难 (Optimization Difficulty)** 的问题。也就是说，这个深层网络我们根本就“没训练好”。
- **为什么优化困难？**
    - 想象一下，你在一片非常崎岖的山地（下图左）里找山谷的最低点，很容易被困在某个小山谷（局部最小值 Local Minimum）出不来。
    - 而残差连接，神奇地将这片山地变得平坦（下图右），让你更容易找到全局的最低点。
- **核心思想：** 增加一个“快捷方式” (Shortcut/Skip Connection)，让输入信息可以直接跳到后面几层，与后面几层的输出相加。这样即使中间几层什么都没学到，模型性能也不会变差，训练过程会稳定得多。
> **小结：** 残差连接的存在理由是 **“让深度神经网络的优化过程变得更容易”**。
## 2. 处理序列：RNN vs. Self-Attention
> **一句话概括：** 为了处理像文字、语音这样的序列数据，诞生了两种主流思路：RNN 像一个“单线程”的读者，一个字一个字地读并更新记忆；而 Self-Attention 像一个能“一目十行”的读者，一次性看完所有文字并找出重点。
我们要处理的问题是：输入一个向量序列 (Vector Sequence)，输出另一个向量序列。
Input: x₁, x₂, ..., xₜ
Output: y₁, y₂, ..., yₜ
在语言模型中，通常输出 `yₜ` 时只能看到它自己和它之前的输入 `x₁` 到 `xₜ`。
### 2.1 RNN 流派 (RNN-style) - 逐步迭代的记忆
> **一句话概括：** RNN 的核心是维护一个叫“隐藏状态”的记忆单元，它不断地吸收新输入，并更新自己的记忆。
- **核心机制：**
    - **隐藏状态 (Hidden State), `h`：** 就像是 RNN 的“短期记忆”，它存储了到目前为止所有输入信息的摘要。
    - **更新过程：** 每一步的记忆 `hₜ`，都由 **上一步的记忆 `hₜ₋₁`** 和 **当前的新输入 `xₜ`** 共同决定。
- **通用公式：**
    ht​=fA​(ht−1​)+fB​(xt​)yt​=fC​(ht​)
    - `hₜ`: 第 `t` 个时间点的隐藏状态 (记忆)。
    - `xₜ`: 第 `t` 个时间点的输入。
    - `f_A`: **反思 (Reflection)** 模块，决定如何处理过去的记忆。
    - `f_B`: **书写 (Write)** 模块，决定如何把新信息写入记忆。
    - `f_C`: **读取 (Read)** 模块，决定如何从记忆中提取信息来生成输出。
- **进化版 RNN (LSTM/GRU)：**
    - 你可能会想，`f_A`, `f_B`, `f_C` 如果一成不变也太死板了。能不能让它们变得“智能”一点？
    - **LSTM (Long Short-Term Memory)** 和 **GRU (Gated Recurrent Unit)** 就是这样做的。它们引入了 **“门 (Gate)”** 的概念，让 `f_A`, `f_B`, `f_C` 的行为可以根据当前的输入 `xₜ` 动态变化。
        - **遗忘门 (Forget Gate, 对应 `f_A`)：** 看到句号，就决定清空记忆，开始新的段落。
        - **输入门 (Input Gate, 对应 `f_B`)：** 看到一个不重要的词（如“的”），就决定不让它进入记忆。
        - **输出门 (Output Gate, 对应 `f_C`)：** 决定当前记忆中的哪些部分对于生成下一个词最重要。
    - > **误区澄清：** 很多人觉得 RNN 的记忆 (`h`) 是一个向量，所以容量很小。其实 `h` 完全可以是一个巨大的矩阵，它的记忆容量可以非常大！
### 2.2 Self-Attention - 一览无余的全局视野
> **一句话概 quát：** Self-Attention 允许每个位置的输出，都能直接“关注”到输入序列中的任何一个位置，并根据重要性（注意力权重）来加权求和，从而获得信息。
- **核心机制 (以计算 `yₜ` 为例)：**
    1. **生成 Q, K, V：** 每个输入 `xᵢ` 都通过乘以三个不同的转换矩阵，生成三个向量：
        - **Query (`qᵢ`):** 代表“我（当前位置）想查找什么信息”。
        - **Key (`kᵢ`):** 代表“我（这个位置）有什么信息可供查询”。
        - **Value (`vᵢ`):** 代表“我（这个位置）实际携带的内容”。
    2. **计算注意力分数 (Attention Score)：** 用当前位置的 Query (`qₜ`) 和 **所有** 位置的 Key (`k₁`, `k₂`, ..., `kₜ`) 做内积 (Inner Product)，得到一个分数 `αₜᵢ`。这个分数衡量了位置 `t` 对位置 `i` 的关注程度。
        αti​=qt​⋅ki​
    3. **归一化 (Softmax)：** 将所有分数 `α` 通过 Softmax 函数，使其总和为 `1`。这样分数就变成了权重，代表了注意力的分配比例。
    4. **加权求和 (Weighted Sum)：** 用归一化后的权重去乘以对应位置的 Value (`vᵢ`)，然后把它们全部加起来，就得到了最终的输出 `yₜ`。
        yt​=i=1∑t​softmax(αti​)⋅vi​
- **历史小知识：** Attention 概念并非始于 `$2017$` 年的著名论文 _Attention Is All You Need_。早在 `$2014$` 年（史前寒武纪！），像 Neural Turing Machine 和 Memory Network 等工作中就已经出现了 Attention 的思想。2017 年的论文贡献在于证明了 **“光有 Attention 就足够了 (Attention is all you need)”**，可以完全抛弃 RNN。
### 2.3 推理 (Inference) 过程大比拼
> **一句话概括：** 在生成文本时，RNN 每一步的计算量和内存是固定的，而 Self-Attention 随着序列变长，计算量和内存需求会急剧增加。
- **RNN：**
    - 要生成 `y₆`，只需要 `h₅` 和输入 `x₆`。它不需要记住 `x₁` 到 `x₅` 的原始信息，因为这些信息已经被压缩进了 `h₅`。
    - **优点：** 计算量和内存占用恒定，对长序列友好。
    - **缺点：** `h₅` 这个“记忆摘要”可能会丢失很久以前的重要信息，造成“遗忘”。
- **Self-Attention (Transformer)：**
    - 要生成 `y₆`，需要对 `x₁` 到 `x₆` **所有** 的 `k` 和 `v` 进行计算。
    - **优点：** 理论上不会丢失任何历史信息，可以直接关注到最开头的词。
    - **缺点：** 序列越长，要存储的 `k` 和 `v` 就越多，计算量也越大（序列长度的平方级）。这在处理超长文本时是致命的。
> **思考题：** 既然 Self-Attention 在推理时又慢又耗内存，为什么它能取代 RNN 统治深度学习领域这么多年？
## 3. Transformer 的真正优势：训练时的并行化
> **一句话概括：** Transformer 最大的革命性优势在于，它在训练时可以像处理一张图片一样，一次性并行计算出整个句子所有位置的输出，极大地利用了 GPU 的并行计算能力。
- **RNN 的训练瓶颈：**
    - 在训练时，即便我们一次性给了 RNN 整个句子 `x₁` 到 `x₆`，它也必须 **按顺序** 计算：`h₀` -> `h₁` -> `h₂` -> ... -> `h₆`。
    - 这种“必须等上一步算完才能算下一步”的依赖关系，是 **GPU 最讨厌的**。GPU 擅长同时做一万件简单的事，而不是按顺序做六件有依赖的事。这导致 RNN 无法有效利用 GPU 的性能，训练速度很慢。
- **Transformer 的并行魔法：**
    - 当一次性给 Transformer 整个句子时，计算任何一个位置的输出 `yᵢ` 所需的 `qᵢ`, `kᵢ`, `vᵢ` 都只依赖于输入 `xᵢ`。
    - 计算 `y₁`, `y₂`, ..., `y₆` 的过程彼此独立，可以 **完全并行**！
    - 这整个过程可以被完美地表示为一系列的 **矩阵运算**。从输入 `X` 到生成 `Q, K, V`，再到计算注意力矩阵，最后乘以 `V` 得到输出 `Y`，每一步都是 GPU 喜欢的操作。
> **结论：** 人们在 `$2017$` 年到最近选择了 Self-Attention，是因为它 **“用推理时的慢，换来了训练时的快”**。在当时，这是一个划算的买卖。
## 4. 新的挑战与 RNN 的复兴
> **一句话概括：** 随着模型需要处理的上下文越来越长（例如百万级别的 token），Self-Attention 在推理时的劣势变得不可接受，人们开始重新思考：RNN 真的不能并行训练吗？
### 4.1 从 RNN 到 Linear Attention 的推导
> **一句话概括：** 通过去掉 RNN 公式中的“反思”项 `f_A`，我们惊奇地发现，这个简化版的 RNN 在数学上等价于一个没有 Softmax 的 Self-Attention，因此它既有 RNN 的推理效率，又能像 Transformer 一样并行训练！
我们来一场头脑风暴，看看能不能让 RNN 并行起来。
1. **写出 RNN 的依赖关系：**
    h1​=fA​(h0​)+fB​(x1​)h2​=fA​(h1​)+fB​(x2​)=fA​(fA​(h0​)+fB​(x1​))+fB​(x2​)...
    问题就出在 `f_A` 的层层嵌套上，它造成了无法并行的计算链。
2. 一个大胆的想法：扔掉 f_A！
    我们假设没有“反思”模块，记忆只是简单地累加。
    ht​=ht−1​+fB​(xt​)
    展开这个式子（假设 `h₀ = 0`）：
    ht​=fB​(x1​)+fB​(x2​)+...+fB​(xt​)=i=1∑t​fB​(xi​)
    现在 `hₜ` 的计算变成了对所有 `f_B(xᵢ)` 的求和，这个求和是可以并行优化的 (e.g., Scan/Parallel Scan 算法)。
3. 重新定义 f_B 和 f_C：
    我们用一种特殊的方式来定义书写和读取模块，让它看起来更像 Attention。
    - 书写 f_B： 我们不直接把信息 vₜ 写进去，而是写一个外积 vₜ kₜᵀ。这是一个矩阵，vₜ 是要写入的 内容，kₜ 是要写入的 地址。
        fB​(xt​)=vt​ktT​
    - 读取 f_C： 我们用一个查询 qₜ 去乘以记忆矩阵 hₜ。
        yt​=fC​(ht​)=ht​qt​
4. 见证奇迹的时刻：
    把这些定义代入 yₜ 的公式：
    yt​=ht​qt​=(i=1∑t​vi​kiT​)qt​
    利用矩阵乘法的结合律，我们可以把括号里的 `qₜ` 移进去：
    yt​=i=1∑t​vi​(kiT​qt​)
    - `k_i^T q_t` 是什么？一个 key 向量的转置乘以一个 query 向量，结果是一个 **标量 (Scalar)**！这不就是我们熟悉的 **Attention Score** 吗？
    - 整个式子是什么？用每个位置的 Attention Score 作为权重，对相应的 Value `vᵢ` 进行 **加权求和**！
> 最终发现：
> 
> Linear Attention = RNN (去掉 f_A) = Self-Attention (去掉 Softmax)
> 
> 这三个看似不同的东西，在数学上竟然是等价的！这意味着我们找到了一个完美的架构：
> 
> - **推理时**，它像 RNN 一样，计算和内存恒定。
>     
> - **训练时**，它像 Transformer 一样，可以完全并行。
>     
### 4.2 Linear Attention 为何最初失败了？
> **一句话概括：** Linear Attention 输给了 Transformer，关键在于它缺少了 Softmax，导致其“记忆”是死板的、只增不减的，而 Softmax 赋予了 Transformer 一种动态的、相对的“注意力”机制。
- **Linear Attention 的问题：永不遗忘。**
    - 在 `h_t = h_{t-1} + v_t k_t^T` 这个公式里，信息一旦通过 `v_t k_t^T` 被加进 `h` 矩阵，就再也无法被修改或遗忘。它会永远待在那里。
- **Softmax 的魔力：相对重要性。**
    - Softmax 的计算是作用于 **整个序列** 的所有 Attention Score 上的。这意味着，一个事件的重要性是 **相对的**。
    - **例子（来自《我独自升级》）：**
        - 刚开始，你觉得暗影君主里的“艾恩”很强（Attention Score = `1.0`）。
        - 后来出现了“尖牙”，比艾恩更强（Score = `2.0`），经过 Softmax 后，艾恩的重要性就被稀释了，你更关注尖牙。
        - 最后“贝尔”登场（Score = `10.0`），尖牙也变得不那么重要了。
    - Softmax 使得注意力可以根据全局上下文动态调整，重要的信息会被凸显，不重要的信息会被抑制，这是一种隐式的“记忆管理”。
### 4.3 进化之路：让 RNN 学会遗忘
既然问题出在“永不遗忘”，那我们就给它加上遗忘机制！
1. **Retention Network (RetNet):**
    > **一句话概括：** RetNet 在 Linear Attention 的基础上，给过去的记忆乘上一个衰减因子，让记忆随时间慢慢淡忘。
    ht​=γht−1​+vt​ktT​
    - `γ` (Gamma) 是一个小于 `$1$` 的常数。这就像我们的记忆，如果不去复习，就会随着时间慢慢模糊。
2. **Gated Retention / Mamba v2:**
    > **一句话概括：** Gated Retention 让衰减因子 `γ` 变成可学习、随时间变化的 `γₜ`，让模型自己决定何时遗忘、何时铭记。
    ht​=γt​ht−1​+vt​ktT​
    - `γₜ` 由当前输入 `xₜ` 决定。模型看到换行符，可能就学会将 `γₜ` 设为接近 `$0$`，从而“清空记忆”，开启新篇章。
3. **Mamba v1:**
    > **一句话概括：** Mamba v1 是一种更复杂的结构，它基于状态空间模型 (State Space Model)，设计了复杂的选择性输入和遗忘机制，是第一个在性能上真正能与 Transformer 抗衡的 RNN 架构。
    - Mamba 的成功之处在于，它设计的复杂操作虽然不能直接展开为矩阵乘法，但可以通过高效的并行扫描 (Scan) 算法在 GPU 上加速训练，兼顾了性能和效率。
4. **DeltaNet:**
    > **一句话概括：** DeltaNet 将 RNN 的记忆更新过程，巧妙地重新诠释为一次“梯度下降”优化，让记忆更新有了更强的理论依据。
    - 它认为，每一次更新记忆 `h`，都是在最小化一个损失函数，这个损失函数的目标是“确保我刚存进去的信息 `vₜ`，能用对应的地址 `kₜ` 再次准确地取出来”。这让记忆的写入和更新过程变得非常直观和优雅。
## 5. 结论与展望
> **一句话概括：** 以 Mamba 为代表的新一代 RNN 架构，成功地结合了 RNN 的推理效率和类似 Transformer 的训练并行性，成为了 Transformer 强有力的竞争者，未来的架构之争充满了悬念。
- **Mamba is Real:** Mamba 及其变体已经成功应用于百亿甚至千亿参数级别的大语言模型中，证明了其可行性和强大性能。
- **应用拓展：** 这类架构也被用于图像生成等领域，在某些任务上展现出比 Transformer 更高的效率。
- **未来趋势：** “从零训练一个大模型”成本太高，一个热门的研究方向是，直接拿现有的 Transformer 大模型（如 LLaMA），将其中的 Self-Attention 层替换为 Mamba-like 的结构，然后进行微调，探索新架构的潜力。
> 最后的赌局：
> 
> 机器学习领域有一个著名的赌局：“到 $2027$ 年，Attention 还是不是你所需要的一切 (Is Attention Still All You Need)?”
> 
> 一方是以 Transformer 为代表的现有霸主，另一方是以 Mamba 为代表的新兴挑战者。这个问题的答案，将定义下一个时代的AI架构。让我们拭目以待。