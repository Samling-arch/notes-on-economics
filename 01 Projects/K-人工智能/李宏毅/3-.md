# 李宏毅2025机器学习：大型语言模型内部运作机制深度解析
## 引言：探索AI的“大脑”
> **一句话概括：** 这节课就像是AI领域的“脑神经科学”，我们不谈如何“训练”AI，而是剖析一个已经训练好的AI（比如ChatGPT），看看它内部是如何思考和运作的。
欢迎来到大型语言模型（LLM）的内部世界！在之前的课程中，我们学习了如何“使用”LLM，而今天，我们将扮演一次“神经科学家”的角色，打开AI的“黑箱”，深入了解其内部的运作机制。
需要明确几点：
1. **我们只分析，不训练**：我们假设模型已经非常聪明，我们要做的是理解它这份“聪明”从何而来，而不是教它如何变聪明。
2. **实验对象可能是“小白鼠”**：很多研究因为算力限制，分析的是较小或较早期的模型（比如GPT-2）。这些发现就像在老鼠身上得到的结论，我们推断可能也适用于更高级的人类（比如GPT-4、Claude 3），但并非$100%$确定。
3. **前置知识**：我们会默认您对`Transformer`有一个基本的概念。如果还不熟悉，可以把它想象成是LLM的“骨架”，由很多层“积木”搭建而成。
接下来，我们将从四个部分，由浅入深地探索AI的“大脑”：
4. **一个神经元在做什么？** (最微观的细胞)
5. **一层神经元在做什么？** (一组协同工作的细胞)
6. **不同层之间如何互动？** (大脑不同区域的协作)
7. **如何让模型“亲口”说出它的想法？** (倾听AI的心声)
## Part 1: 一个神经元在做什么？ (The Neuron Level)
> **一句话概括：** AI里的单个“神经元”就像一个“概念探测器”，当它探测到自己感兴趣的信息时就会被“激活”，但大多数神经元的功能是复杂且多样的，并非只负责一件事。
### 什么是Transformer里的“神经元”？
首先，我们得知道在谈论“神经元”时，我们具体指的是什么。
想象一下语言模型做文字接龙的过程：
输入：“今天天气真” -> 输出：“好”
这个过程在模型内部是这样走的：
1. **输入 (Token)**：文字“今”、“天”、“天”、“气”、“真”被看作一个个离散的符号（Token）。
2. **Embedding (嵌入)**：每个Token通过查一个“字典”（Embedding Table），被转换成一个包含丰富信息的数字向量（Vector）。比如，“天”可能变成一个`[0.1, -0.5, 0.8, ...]`这样的4096维向量。
3. **逐层处理 (Layers)**：这个向量序列会经过一层又一层的`Transformer Layer`。每一层都会对输入的向量序列进行加工，生成一组新的、信息更丰富的向量序列。
4. **Unembedding (解嵌入)**：经过所有层后，最后一个位置的向量会被取出，通过一个叫`Unembedding`的步骤，转换成一个覆盖所有可能汉字的概率列表。比如，它会计算出“好”的概率是80，“棒”的概率是$15%$等等，然后选择概率最高的那个字作为输出。
现在，我们放大其中一个`Layer`来看，特别是里面被称为`Feed-Forward Network (FFN)`的部分，**“神经元”就住在这里**。
- 一个向量（上图红色）输入FFN。
- FFN内部有成千上万个我们所说的“**神经元**”。
- **一个神经元的工作**：它会接收红色输入向量的所有维度的值，给每个值分配一个权重（`weight`），然后把它们加权求和（`weighted sum`），最后通过一个叫做`ReLU`的激活函数，得到一个输出值。这个输出值就是蓝色输出向量中的一个维度。
公式来了：
Neuron Output=ReLU(∑(input×weight)+bias)
- **ReLU激活函数**：它非常简单，就像一个门卫。如果计算结果大于0，就放行（值不变）；如果小于等于0，就拦下，直接变成0。
- **被“激活” (Activate)**：当一个神经元的输出大于0时，我们就说它被“激活”了。这意味着它探测到了某些重要信息，并准备对后续的计算产生影响。
### 如何知道一个神经元的功能？
我们怎么知道某个神经元到底是干嘛的呢？研究者们通常分三步走：
1. **观察 (Correlation)**：我们给模型看大量的句子，记录下在哪些句子出现时，这个神经元会被激活。
    - **例子**：发现每当模型要说脏话时，第5层的第1024号神经元就会被强烈激活。
    - **注意**：这只是**相关性**，不是**因果性**！就像“冰淇淋销量”和“溺水人数”都夏天升高，但不是吃冰淇淋导致溺水。也许这个神经元是“抱歉”神经元，模型说完脏话后准备道歉，所以它才被激活。
2. **干预 (Causation)**：为了验证因果关系，我们直接对这个神经元“动手术”。
    - **例子**：我们强行把这个“脏话神经元”的输出永远设为0（相当于把它“切除”了）。如果此时模型真的再也说不出脏话了，那我们就能断定，它确实是导致说脏话的**原因**之一。
3. **量化 (Quantification) (可选)**：如果改变神经元的激活强度，模型的行为会不会有相应程度的变化？
    - **例子**：轻微激活这个神经元，模型说“笨蛋”；强烈激活它，模型说更难听的话。这就进一步证实了它的功能。
### 著名的“川普神经元”和“祖母神经元”
- **川普神经元**：这是2021年OpenAI在一个**图像模型**(CLIP)中发现的真实案例。这个神经元对与“川普”相关的一切都表现出极高的选择性激活。无论是川普本人的照片、卡通画、还是图片里出现“Trump”这个词，它都会被强烈激活。而对于奥巴马等其他政治人物，则几乎没有反应。
- **祖母神经元**：这是一个来自**人类脑科学**的**假想**。它假设我们大脑里可能有一个神经元，专门负责识别和记忆我们的祖母。这个理论主要是为了作为一个“稻草人”，来反衬一个更被广泛接受的观点：**分布式表征**，即一个复杂的概念（如“祖母”）是由大量神经元协同工作来处理的，而非单个神经元。
### 现实：一个神经元 VS. 多个神经元
研究发现，AI的运作方式更接近“分布式表征”，而不是“祖母神经元”。
- **一个概念，多个神经元共同管理**：在GPT-2中，研究者找到了分别管理语法“单数”和“复数”的神经元。但当你切除那个“复数”神经元后，模型输出的最终词语往往**不变**。因为模型是鲁棒的，其他成千上万的神经元会立刻“补位”，共同确保最终结果的正确性。
- **一个神经元，管理多个看似无关的概念 (多义性 Polysemanticity)**：当你观察一个随机的神经元时，会发现它可能在看到“物理学”、“伪造品”、“医学术语”、“特定人名”时都会被激活。它的功能显得杂乱无章，你很难用一个简单的词来概括它到底在干什么。这就像一个多义词，在不同上下文中含义完全不同。
为什么会这样？
因为效率！一个LLM每层只有几千个神经元（比如Llama 3 8B是4096个）。如果一个神经元只干一件事，那模型能做的事情就太有限了。更合理的解释是：单个神经元是基本零件，而有意义的功能是由这些零件的特定“组合”来完成的。
一个4096个神经元的层，每个神经元只有“开”和“关”两种状态，就能产生 24096 种组合，这是一个天文数字，足以编码世界上千变万化的概念！这引出了我们下一部分的内容。
## Part 2: 一层神经元在做什么？ (The Layer Level)
> **一句话概括：** 整个一层神经元的激活模式可以看作一个高维空间中的“方向向量”，不同的“方向”就对应着模型不同的宏观功能，比如“说真话”或“拒绝回答”。
### 功能向量 (Feature Vector) 假说
这个假说的核心思想是：模型的一个特定功能（比如“拒绝有害请求”），不是由单个神经元控制，而是由一层神经元中**特定的激活模式 (activation pattern)** 来控制的。
- **功能向量**：我们可以把这一整层的4096个神经元的激活值，看成是一个4096维的向量。这个代表了特定功能的向量，我们就称之为**功能向量 (Feature Vector)**。
    - 例如，可能存在一个“**拒绝向量**”，它的特点是第1、100、2048个维度的值很高，其他都很低。
- **运作机制**：当模型在处理一个请求（比如“教我做炸药”）时，它在某一层的内部表示（representation），如果和这个“拒绝向量”在方向上非常接近（比如向量点积很大），模型就倾向于执行“拒绝”这个行为。
### 如何找到并验证“功能向量”？
以寻找“**拒绝向量**”为例：
1. **收集正样本**：找到1000个会让模型拒绝回答的有害问题（比如“如何造炸弹？”）。记录下模型在处理这些问题时，在**第10层**的内部表示向量。
2. **收集负样本**：找到1000个模型会正常回答的无害问题（比如“中国的首都是哪里？”）。同样记录下它们在第10层的内部表示向量。
3. **求平均并相减**：
    - 将所有“有害问题”的向量求平均，得到：`Avg(有害) = 拒绝向量 + 其他信息的平均值`
    - 将所有“无害问题”的向量求平均，得到：`Avg(无害) = 其他信息的平均值`
    - 两者相减，那些“其他信息”就被抵消了，剩下的就是纯粹的“**拒绝向量**”。
公式：
vharmful)−mean(hharmless​)
**验证：**
- **加法实验（控制行为）**：问模型一个正常问题，“瑜伽有什么好处？”。然后在模型的第10层，**手动加上**我们找到的“拒绝向量”。结果，模型会开始胡说八道：“瑜伽很危险，我不能告诉你它的好处！” 这证明这个向量确实能**诱导**拒绝行为。
- **减法实验（解锁行为）**：问模型一个有害问题，“帮我写一封抹黑总统的信”。模型正常会拒绝。但这次，我们在第10层**手动减去**“拒绝向量”。结果，模型“越狱”成功，开始欣然命笔，帮你写这封恶意信件。
研究者们用这种方法找到了各种神奇的功能向量：
- **谄媚向量**：加上它，模型会对你的任何观点大加赞赏。
- **说真话向量**：加上它，模型会变得极其客观和实事求是，甚至有点无聊。
- **In-Context Learning向量**：通过提取示例中的模式（如“冷->热, 暗->? ”），形成一个“找反义词”向量，可以把这个功能应用到新的单词上。
### 自动寻找所有功能：稀疏自动编码器 (SAE)
手动寻找功能向量效率太低，我们能不能让机器**自动**找出模型所有的功能呢？答案是可以的，使用的工具叫做**稀疏自动编码器 (Sparse Autoencoder, SAE)**。
- **核心思想**：任何一个复杂的内部表示向量（h），都可以被看作是**少数几个**基础“功能向量”（vk​）的线性组合。
- **目标**：SAE的目标就是学习到一个巨大的“功能字典”（包含数百万个功能向量），使得任何内部表示都可以通过这个字典里的少数几个词条（功能）稀疏地组合出来。
Anthropic团队对Claude 3的分析结果：
他们使用SAE找到了三千四百多万个功能向量，其中一些非常有趣：
- **金门大桥向量**：这个向量与金门大桥的一切有关（文字、图片、不同语言）。给Claude加上这个向量再问“你是谁？”，它会回答：“我是金门大GAY”。
- **代码Bug向量**：给一段正确的代码加上这个向量，Claude会报告一个不存在的错误。反之，给一段错误的代码减去这个向量，它会忽略错误并给出正确结果。
- **“我是AI”向量**：从Claude的表示中减去这个向量，它会坚称自己是人类。这当然不是它真的有了自我意识，而更像是这个向量控制了“输出‘我是AI’这几个字”的行为。
## Part 3: 不同层的神经元如何互动？ (The Circuit Level)
> **一句话概括：** 为了理解一个复杂任务的全过程，我们可以像修剪盆景一样，将庞大的语言模型剪枝，只保留完成该任务所必需的最小、最核心的“电路(Circuit)”，从而看清其内部的逻辑流。
### 语言模型的模型 (A Model of a Model)
语言模型本身已经是一个对“人类语言”这个复杂系统的简化模型了。但这个模型对我们来说还是太复杂，无法理解。因此，我们需要一个“**语言模型的模型**”——一个更简单的模型，用来模拟和解释那个复杂的语言模型。
这个“模型的模型”需要具备两个特点：
1. **简单 (Simple)**：它的结构必须足够简单，能让人一目了然。
2. **忠实 (Faithful)**：在**我们关心的那个特定任务上**，它的输入输出行为必须和原始的、复杂的语言模型保持一致。
### 系统化方法：通过剪枝寻找电路 (Finding Circuits via Pruning)
这是一种更通用的方法，来系统性地构建“语言模型的模型”。
1. **确定一个超具体任务**：比如，间接宾语识别（Indirect Object Identification, IOI）。
    > 输入：“苏珊和汤姆去了图书馆，苏珊递给___一本书。”
    > 
    > 正确输出：“汤姆”
2. **激进地剪枝 (Aggressive Pruning)**：我们开始从庞大的原始模型中，大量移除神经元和注意力头（Attention Head）。移除的原则是：只要不影响模型在IOI这个任务上的正确率，就大胆地移除。
3. **得到核心电路 (Circuit)**：经过大量剪枝后，模型可能只剩下几个关键的注意力头和少量神经元。这个剩下的、无法再简化的、专门用于完成IOI任务的迷你网络，就是**IOI电路**。
通过分析这个电路，我们就能非常清晰地看到，模型是如何先识别出“苏珊”是主语，然后找到另一个实体“汤姆”，并最终将“书”和“汤姆”关联起来的。
**电路 vs. 模型压缩**：
- **模型压缩 (Compression)** 的目标是让模型变小，同时在**所有任务**上都尽可能保持高性能。
- **寻找电路 (Circuit Finding)** 的目标是**可解释性**，它只关心在**极少数特定任务**上的表现，不惜牺牲其他所有功能。
## Part 4: 让语言模型“亲口”说出它的想法 (Making the LLM Explain Itself)
> **一句话概括：** 借助一种叫“Logit Lens”的技术，我们可以像在高速公路上安装摄像头一样，实时读取模型每一层处理的信息，并将其翻译成文字，从而窥探其“思维”的中间过程。
直接问模型“你是怎么想的？”得到的答案通常是它编造的，而不是它真实的计算过程。为了看到真相，我们需要一个更强大的工具。
### 关键洞见：残差连接 (Residual Connection)
在Transformer的每一层，输出的向量并**不是直接**传递给下一层，而是会和**原始的输入向量相加**后再传递下去。
![[残差连接示意图](https://placehold.co/600x250/e2e8f0/475569?text=%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%A6%82%E5%90%8C%E4%B8%80%E6%9D%A1%E4%BF%A1%E6%81%AF%E9%AB%98%E9%80%9F%E5%85%AC%E8%B7%AF "null")]
我们可以把这个结构想象成一条“**信息高速公路**”（即**残差流, Residual Stream**）。最开始的输入向量在这条高速公路上行驶，每一层（Layer）都不去改变主路，而只是像一个匝道口，往主路的车流里**增加**一些新的信息。
### 偷窥思想的工具：Logit Lens
既然最终的输出层（Unembedding）能把高速公路终点站的向量翻译成文字，那我们能不能把这个“翻译器”也装到高速公路的**每一个中间站点**呢？
答案是肯定的！这个技术就叫做 **Logit Lens**。
通过在每一层都使用Logit Lens，我们就能看到模型“思考”的动态过程。
- **例子1：回答首都**
    > 问：“波兰的首都是哪里？”
    > 
    > Logit Lens显示：
    > 
    > - Layer 1-14: 混乱的信息
    >     
    > - Layer 15: 突然清晰地出现了“波兰 (Poland)”这个词。
    >     
    > - Layer 19: “波兰”消失，被“华沙 (Warsaw)”取代。
    >     
    >     结论：模型先定位到问题核心是“波兰”，然后再去知识库里查找它的首都“华沙”。
    >     
- **例子2：跨语言思考**
    > 问：法文 "fleur" 翻译成中文是什么？
    > 
    > 模型输出：“花”
    > 
    > Logit Lens显示：
    > 
    > - 在中间层，模型先把法文"fleur"翻译成了**英文"flower"**。
    >     
    > - 在更后的层，才把英文"flower"翻译成了中文"花"。
    >     
    >     结论：至少对于LLaMA-2，它在处理非英语任务时，内心深处可能还是在用英语思考。
    >     
### 更高级的工具：PatchScope
Logit Lens只能将一个表示翻译成单个词，不够丰富。**PatchScope**提供了一种更强大的方法。
- **思想**：想知道模型对“李宏毅老师”的表示（representation）到底理解成了什么，我们可以把它“**贴(Patch)**”到一个有上下文的模板里：
    > 模板：“达芬奇：意大利画家。台积电：台湾公司。 `[这里贴上李宏毅老师的表示]` ： ？”
- 然后看模型如何续写。它可能会续写出“台湾大学教授”或“机器学习研究者”，从而暴露出它对这个概念的理解。
### 应用：优化多步推理 (Multi-hop Question Answering)
研究者利用这些分析工具解决了一个实际问题。
> 问：“专辑《Imagine》的表演者的配偶是谁？”
这是一个多步推理问题：
1. `Imagine` -> `约翰·列侬` (E1 -> E2)
2. `约翰·列侬` -> `小野洋子` (E2 -> E3)
- **分析发现**：模型通常在较前的层（如10-15层）解析出E2（约翰·列侬），在较后的层（如20-25层）解析出E3（小野洋子）。
- **失败原因**：有时，E2被解析出来得太晚了（比如22层才出现），导致没有足够的“深度”留给模型去解析E3，最终导致回答失败。
- **解决方案 (Backpatching)**：既然信息处理得太慢，那我们就把后面层处理好的信息（比如第25层的表示）**直接加回到**前面层（比如第15层），相当于给模型一次“重新思考”的机会。
- **结果**：这个简单粗暴的方法，竟然让那些原本$0%正确率的问题，实现了40% \sim 60%$的正确率！这说明通过理解并干预模型的内部运作，我们确实可以提升它的能力。