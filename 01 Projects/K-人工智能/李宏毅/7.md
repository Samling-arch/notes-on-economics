%% 本文档的整体知识框架 %%
## 0.1 💡 整体知识框架：深度思考大语言模型之旅
```mermaid
graph TD
    A[开始：深度思考大语言模型] --> B{什么是深度思考?};
    B --> C[深度思考的表现与作用];
    C --> D[核心概念：Test-Time Compute / Scaling];
    D --> E[如何打造深度思考模型?];
    E --> E1[方法一：更好的Chain-of-Thought (CoT)];
    E --> E2[方法二：直接给定推理工作流程];
    E --> E3[方法三：模仿学习 (Imitation Learning)];
    E --> E4[方法四：结果导向学习 (Reinforcement Learning)];
    E --> F[案例分析：DeepSeek-R1 的打造过程];
    F --> G[挑战与未来发展];
    G --> H[结束：成为专家];
```
## 0.2 第一讲：什么是大语言模型的“深度思考”（Reasoning）？
> 💡 **通俗概括：** 大语言模型不再是只给答案的“计算器”，它现在还会像人类一样，在心里“打草稿”，告诉你它是怎么一步步想出答案的。
### 0.2.1 概念：大语言模型的“深度思考”
深度思考，英文通常称为 **==Reasoning==**，指的是当前一些大型语言模型展现出的一种==特定的行为模式==：在给出最终答案之前，它们会先展示一个详细的、分步骤的思考过程，就像在进行一场“内心小剧场”。
- **区别于传统“推论”（Inference）：**
    - **Inference（推论）：** 在机器学习领域，通常指你==使用一个模型来产生答案的过程==。它强调的是模型从输入到输出的直接计算过程。
    - **Reasoning（推理/深度思考）：** 特指在进行Inference时，模型==产生了一个特别长的思考过程==。这并不是说模型真的像人一样“思考”，而是它表现出了一种类似思考的、多步骤的生成行为。
### 0.2.2 哪些模型拥有“深度思考”能力？
现在许多领先的大型语言模型都具备这种能力，例如：
- **ChatGPT的O系列** (例如：GPT-4o)
- **DeepSeek的R系列** (例如：DeepSeek-R1)
- **Gemini的Flash Thinking**
- **Claude 3.7**
- **Sony的Extended Thinking Mode**
### 0.2.3 “深度思考”看起来像什么？—— DeepSeek-R1的“内心小剧场”
以DeepSeek-R1为例，当你问它一个简单的问题，比如“1 + 1 等于多少？”时，除了直接给出答案“2”之外，它还会弹出一个框框，显示“以深度思考”。如果你点开这个框框，就会看到它“内心”的思考过程：
> **模型内心小剧场：**
> 
> 1. “首先，一个苹果加一个苹果，就是两个苹果，所以1 + 1 = 2。”
>     
> 2. “等一下，这会不会有什么陷阱？让我想想看，1 + 1 会不会不一定等于二？比如说在二进制中，1 + 1 = 10。我想，好聪明哦，我问他这个问题的时候，我怎么都没有想到，其实1 + 1 不一定等于二。”
>     
> 3. “使用者只是写了1 + 1，没有提供额外的资讯，他可能只是测试我会不会想太多，所以最后的答案是2。”
>     
这个例子生动地展示了模型的“深度思考”：它会==自我验证==、==探索其他可能性==，甚至==修正自己的想法==。
### 0.2.4 技术实现细节：`think` 和 `/think` 符号
通常，这些模型的思考过程会被包裹在特定的符号之间，例如 `think` 和 `/think`。
- **作用：** 这主要是为了==方便用户界面（UI）的呈现==。开发者可以选择是否将这些符号内部的内容展示给用户。这样，模型输出的完整性得到了保持，同时用户也可以选择性地查看思考过程，避免信息过载。
- **模型行为：** 在这个思考过程中，模型通常会表现出以下几种行为：
    - **验证（Verification）：** “Let me check the answer.”（让我检查一下刚才我自己想的答案是不是对的。）
    - **探索（Exploration）：** 尝试思考其他的可能性。
    - **规划（Planning）：** 规划解决问题所需的步骤。
## 0.3 第二讲：核心概念：Test-Time Compute (TTC) 与 Test-Time Scaling (TTS)
> 💡 **通俗概括：** 就像考试前多花时间打草稿、多检查几遍就能考更好一样，大语言模型在回答问题时多“想”一会儿，多“算”一会儿，也能给出更好的答案。
### 0.3.1 Test-Time Compute (TTC) - 测试阶段的算力投入
- **定义：** ==Test-Time Compute (TTC)== 意思是在模型进行==测试（Inference）阶段==，你投入了==更大的计算能力（算力）==，而这种额外的算力投入，通常可以让你得到==更好的结果==。
- **与Reasoning的关系：** 深度思考（Reasoning）就是一种TTC的应用，它通过生成冗长的思考过程来投入更多的算力。
- **核心思想：** 这印证了我们之前课程中提到的一个观点：==“深度不够，长度来凑”==。当模型本身的“思考深度”不足以直接解决复杂问题时，通过延长其思考的“长度”（即更详细的推理过程），可以弥补这种不足。
### 0.3.2 历史回顾：AlphaGo 与蒙特卡洛树搜索 (MCTS)
TTC并非全新的概念，早在“寒武纪时代”的AlphaGo中就已经有所应用：
```
graph TD;
    A[AlphaGo (围棋AI)] --> B{训练阶段};
    B --> B1[Policy Network: 决定下一步落子];
    B --> B2[Value Network: 评估盘面赢面];
    B --> C{测试阶段 (决定落子)};
    C --> C1[Monte Carlo Tree Search (MCTS): 巨大的运算];
    C1 --> C2[脑内小剧场: 模拟多步棋局, 预测胜率];
    C2 --> C3[选择胜率最高的位置];
    C3 --> D[最终落子];
```
- **Policy Network vs. MCTS：** AlphaGo在训练时会训练一个Policy Network来决定下一步落子，但它在实际对弈（测试）时，并==不直接使用Policy Network的决策==。Policy Network只提供几个可能的选项作为参考。
- **MCTS 的巨大算力投入：** 在测试阶段，AlphaGo会利用==蒙特卡洛树搜索（MCTS）==来模拟“脑内小剧场”，假设在某个位置落子后接下来会发生什么，并预测胜率。MCTS是一个==非常耗费算力的过程==，但正是这种测试阶段的算力投入，让AlphaGo能够找到真正最优的落子点。
### 0.3.3 Test-Time Scaling (TTS) - 思考越多，结果越好
- **定义：** ==Test-Time Scaling (TTS)== 描述了这样一种现象：如果你在测试阶段投入越多的算力（即让模型进行越多的“思考”），通常就能得到==越好的结果==。
- **早期实验：** 早期的一项研究（非语言模型，关于棋类游戏）就展现了TTC的潜力。
    - **实验设计：**
        - 横轴：训练算力投入 (Training Time Compute)。
        - 纵轴：测试算力投入 (Test Time Compute)。
        - 等分线：代表模型获得相同分数的连接线（分数越接近0越好）。
    - **发现：** 为了达到相同的性能，你可以选择：
        1. 投入大量训练算力（例如训练更大的模型），然后少量测试算力。
        2. 投入少量训练算力，但投入大量测试算力（例如进行更深更广的MCTS搜索）。
    - **结论：** 实验发现，==少量测试算力就能显著减少训练阶段所需的大量算力==，这表明TTC是一个非常有潜力的方向。
```
graph TD;
    A[Test-Time Scaling] --> B[思考越多];
    B --> C[结果越好];
    C --> D[语言模型应用];
    D --> D1[生成冗长思考过程];
    D1 --> D2[解决更难问题];
    D --> E[AlphaGo示例];
    E --> E1[MCTS];
    E1 --> E2[脑内模拟];
```
- **语言模型中的应用：** 如今，TTC和TTS的概念在语言模型中得到了广泛应用。通过产生非常长的思考过程，语言模型能够解决更复杂的问题。
## 0.4 第三讲：如何打造深度思考模型？—— 四大方法详解
> 💡 **通俗概括：** 想要让模型学会“深度思考”，我们可以用四大“法宝”：要么引导它一步步说清楚（像解题步骤），要么让它多试几次找到好答案，要么让它模仿“老师”怎么思考，要么只看结果对不对来奖励它。
打造能进行深度思考的语言模型，目前主要有四种不同类型的方法，其中前两种不需要微调参数，后两种则需要微调参数。
```
graph TD;
    A[如何打造深度思考模型?] --> B{无需微调参数};
    B --> B1[方法一: 更好的Chain-of-Thought (CoT)];
    B --> B2[方法二: 直接给定推理工作流程];
    B --> C{需要微调参数};
    C --> C1[方法三: 模仿学习 (Imitation Learning)];
    C --> C2[方法四: 结果导向学习 (Reinforcement Learning)];
```
### 0.4.1 方法一：更好的 Chain-of-Thought (CoT) - 无需微调参数
> 💡 **通俗概括：** 就是让模型“说话”的时候，像小学老师教数学一样，先写解题步骤，最后再写答案。
- **概念回顾：** ==Chain-of-Thought (CoT)==，简称CoT，是我们之前课程中多次提到的概念。它通过==让模型先列出解题过程，再给出最终答案==的方式，来提升模型的推理能力。
- **发展历程：**
    - **Few-shot CoT (2022年)：** 最早的方式。通过给模型提供一些==范例（demonstrations）==，展示问题如何一步步解决并给出答案，然后模型会模仿这种模式来回答新问题。
        - **示例 Prompt 结构：**
            ```
            问题A: ...
            思考过程: ...
            答案: ...
            问题B: ...
            思考过程: ...
            答案: ...
            现在是新问题: 问题C: ...
            思考过程:
            ```
    - **Zero-shot CoT (几个月后发现)：** 更简洁的方式。无需任何范例，只需在问题前加上一句简单的指令，如 ==“Let's think step by step.” (让我们一步步思考) ==，模型就能自动列出计算过程。
- **Long Chain-of-Thought (LCoT)：**
    - 当前的深度思考模型，其思考过程通常非常长，因此现在将其称为 **==Long Chain-of-Thought (LCoT)==**。而2022年之前的CoT则被称为Short CoT。
    - **区别：** LCoT和Short CoT之间并没有严格的长度界限。要让模型产生冗长的思考过程，关键在于==提供更长、更精确的指令（prompt）==。
- **Supervised CoT：**
    - 这并不是机器学习中传统的“监督学习”，而是指==用人类的知识来“监督”模型如何进行CoT==。
    - 通过编写==复杂的Prompt==，详细指示模型应该如何分析问题、制定计划、执行步骤以及进行验算，从而引导模型进行更深入的思考。
    - **示例 (GPT-4o 的 Supervised CoT)：**
        - **Prompt 示例：**
            ```
            请逐步解决输入的数学问题。在回答问题前，你要先深入解析题目的要求，定出一个完整且清晰的解题计划，明确列出你要如何按部就班完成这个问题。每次执行一个步骤的时候，还要列出那个步骤的子计划。列出子计划以后再按部就班的执行。每次执行完一个步骤或子计划，要做多次的验算，确保你的答案是正确的，而且要考量所有可能的解法。思考的过程通常要放在<think>跟</think>这两个符号之间。
            问题：123 × 456 等于多少？
            ```
        - **模型输出示例（部分节选）：**
            ```
            <think>
            题目分析：需要计算两个三位数123和456的乘积。
            解题总计划：
            1. 将456拆分为400 + 50 + 6。
            2. 依次计算 123 × 400，123 × 50，123 × 6。
            3. 将上述结果相加。
            4. 进行多次验算以确保结果正确。
            验算计划：
            5. 使用标准直式乘法重新计算。
            6. 估算答案的上下界。
            7. (计划了Python验算，但未执行，模型出现幻觉)
            ... (中间计算过程省略) ...
            验算过程：
            8. 直式乘法验算：
               123
             x 456
             -----
               738 (123 × 6)
              6150 (123 × 50)
             49200 (123 × 400)
             -----
             56088 (正确)
            9. 估算答案上下界 (模型在此处出现幻觉，估算逻辑有误，但仍判断结果合理)
            10. 乘法交换律拆解 (模型在此处出现幻觉，尝试后放弃)
            </think>
            最终答案：56088
            ```
        - **观察与局限：**
            - 这种方法能让模型展现出部分深度思考能力，甚至能进行多步骤的验算。
            - **局限性：** 这种方法==只适用于能力较强的模型==（如GPT-4o）。较弱的模型（如Llama 3 8B）可能无法完全理解并执行复杂的指令。
### 0.4.2 方法二：直接给定推理工作流程 - 无需微调参数
> 💡 **通俗概括：** 与其教模型“怎么想”，不如直接告诉它“这样做”：比如让它对同一个问题反复尝试几万次，总有一次能“蒙对”正确答案。
这种方法的核心是==直接强迫模型按照特定的“工作流程”来生成答案==，即使是比较弱的模型也能大幅强化其能力。
- **2.1 “无限猴子”原理 (Large Language Monkeys)：**
    - **概念：** 就像无限多的猴子在无限长的时间内随机打字，最终总能打出莎士比亚全集一样，让语言模型对同一个问题==不断重复生成答案==（每次生成都可能不同），总有机会生成出正确答案。
    - **实验发现：**
        - 横轴：模型解题的尝试次数（1次到10000次）。
        - 纵轴：**Coverage**（只要在模型所有输出中，有一次是正确的，就算正确）。
        - 结果显示，对于Llama 3 70B 和 Llama 3 8B 等模型，==只要尝试次数足够多（例如1万次），总是有机会“赛到”正确答案==（愚者千虑必有一得）。
        - 较强的模型（如70B）可以在更少的尝试次数下达到高覆盖率，而较弱的模型（如PYTHIA 70M）即使尝试多次也难以成功。
    ```
    graph LR;
        A[无限猴子原理] --> B[对同一问题重复生成答案];
        B --> C[每次生成答案都可能不同];
        C --> D[增加生成正确答案的概率];
        D --> E[更强模型 --> 少量尝试次数];
        D --> F[较弱模型 --> 更多尝试次数];
    ```
    - **挑战：** 模型生成了上万个结果，==如何知道哪一个才是正确的答案？==
- **2.2 如何选择正确答案？**
    - **方法A：Majority Vote (多数投票) / Self-Consistency (自洽性)**
        - **原理：** 查看在模型所有生成的答案中，==哪一个答案出现的次数最多==，就认为它是正确的。这是一种简单但非常强大的基线方法。
        - **实现：** 通常需要提前在Prompt中约定好答案的==“特定位置”==（例如：`答案：[数字]`），方便后续提取和统计。
        - **实验结果：** Majority Vote可以显著提高模型的正确率。
            - 即使是Llama 3 1B模型，通过多数投票也能大幅提升性能，但通常仍无法超越Llama 3 8B模型。
    ```
    graph TD;
        A[选择正确答案] --> B[Majority Vote / Self-Consistency];
        B --> B1[多次生成答案];
        B1 --> B2[统计答案出现次数];
        B2 --> B3[选择出现次数最多的答案];
        B[Majority Vote / Self-Consistency] --> B4[通常需要强制模型将答案放置在特定位置];
    ```
    - **方法B：Confidence (置信度) / CoT Decoding**
        - **原理：** 计算模型生成每个答案时的==概率（置信度）==，选择置信度最高的那个答案。
        - 这种方法在一些被称为CoT Decoding的技术中被应用。
- **2.3 中间步骤验证 (Process Verifier)**
    - **问题：** 如果模型只有在完成全部推理后才验证答案，那么一旦第一步就错了，会浪费大量计算资源。
    - **解决方案：** 让模型在==解题的中间步骤就进行验证==，避免“一步错，步步错”。
    - **DeepSeek-R1 示例：** 当计算 `123 × 400` 时，DeepSeek-R1会先计算 `123 × 4` 得到 `492`，然后立即对其进行验算（例如通过 `100×4 + 20×4 + 3×4` ），确认无误后才继续下一步。
    ```
    graph TD;
        A[中间步骤验证] --> B[Process Verifier];
        B --> B1[在解题中途进行验证];
        B1 --> B2[避免一步错步步错];
        B --> C[如何实现中途停止?];
        C --> C1[In-Context Learning (通过Prompt)];
        C1 --> C2[定义步骤标记 (如`step`和`/step`)];
        C2 --> C3[模型生成到`/step`时停止];
    ```
    - **如何让模型“分步”生成？**
        - 通过==In-Context Learning (情境学习)==，即在Prompt中给模型提供演示：
            - **Prompt 示例：**
                ```
                请逐步解决输入的数学问题。每一步骤的开头你都要输出<step>，然后步骤的结尾就输出</step>。
                范例：
                <step>步骤一：...</step>
                <step>步骤二：...</step>
                问题：123 × 456 怎么解？
                ```
            - 通过强制模型在生成到 `</step>` 符号时就停止生成，从而每次只输出一个步骤。
    - **如何得到 Process Verifier (过程验证器)？**
        - **挑战：** 只有问题和最终答案，无法直接判断中间步骤的正确性。
        - **思路：** 通过==多次采样（sampling）==，让模型从一个给定的中间步骤开始，尝试完成后续所有步骤并得到最终答案。
        - **训练 Process Verifier：**
            1. 将问题和==部分（中间）解题过程==作为输入。
            2. 让语言模型从这个中间步骤开始，==多次采样生成剩余的步骤和最终答案==。
            3. 统计这些采样中，最终答案==正确的比例==。
            4. 将这个“正确比例”作为标签，训练一个独立的==Process Verifier 模型==。
            5. 这个Verifier模型可以学会：看到某个中间步骤时，能够输出继续下去得到正确答案的概率。
    ```
    graph TD;
        A[如何得到 Process Verifier?] --> B[训练数据准备];
        B --> B1[输入: 问题 + 部分解题过程 (e.g., Step 1)];
        B --> B2[让语言模型从Step 1多次采样生成后续步骤和最终答案];
        B2 --> B3[根据最终答案是否正确，计算从Step 1开始的正确率 (e.g., 2/3)];
        B --> B4[训练Process Verifier];
        B4 --> B5[输入: 问题 + 部分解题过程];
        B5 --> B6[输出: 继续下去得到正确答案的概率];
    ```
- **2.4 Beam Search (集束搜索)**
    - **问题：** Process Verifier通常输出的是一个概率值，如何设定阈值来决定“正确可能性高到什么程度才继续”？
    - **解决方案：** ==Beam Search==。
        - **原理：** 在每一步，我们==只保留Process Verifier认为最好的N条路径==（“beam”），而放弃其他较差的路径。N是预设的“集束宽度”。
        - **过程：**
            1. 模型为问题生成多个第一步（Step 1）。
            2. Process Verifier评估每个Step 1的潜力。
            3. 保留最好的N个Step 1。
            4. 对于每个保留下来的Step 1，模型继续生成多个Step 2。
            5. Process Verifier评估每个（Step 1 + Step 2）路径的潜力。
            6. 再次保留最好的N个路径，并继续。
        - **实验结果：**
            - Beam Search能够比Majority Vote和Best-of-N方法表现得更好。
            - 甚至在某些情况下，通过Beam Search，==1B的小模型能够超越8B的大模型==，这非常令人惊讶。
    ```
    graph TD;
        A[Beam Search] --> B[概念: 每步保留最佳N条路径];
        B --> B1[避免阈值问题];
        A --> C[流程];
        C --> C1[生成多个Step 1];
        C1 --> C2[Process Verifier评估Step 1];
        C2 --> C3[保留最佳N条Step 1];
        C3 --> C4[从保留的Step 1继续生成Step 2];
        C4 --> C5[Process Verifier评估(Step 1 + Step 2)路径];
        C5 --> C6[保留最佳N条(Step 1 + Step 2)路径];
        C6 --> C7[重复直到得到最终答案];
        A --> D[优势: 1B模型可超越8B模型];
    ```
    - **变形与相关算法：**
        - **DVTS：** Beam Search的变形，旨在保留更多“不相似”的路径，防止过早收敛到局部最优，但效果不一定显著。
        - **启发式搜索算法 (Heuristic Search Algorithms)：** 诸如 A* 算法、蒙特卡洛树搜索 (MCTS) 等都可以与这个框架结合，在大型语言模型推理中发挥作用。大量研究表明MCTS能够带来帮助。
### 0.4.3 方法三：模仿学习 (Imitation Learning) - 需要微调参数
> 💡 **通俗概括：** 就像一个学生跟着老师学解题，老师把思考过程都写下来，学生就照着模仿老师的思考方式。
- **概念：** ==Imitation Learning (模仿学习)==，在这里指的是==直接教模型如何进行推理==。我们给模型一个“老师”（可以是人类或者另一个更强的语言模型），它会模仿老师的推理行为。
- **训练数据：**
    - 通常，训练数据只有问题和答案。但对于模仿学习，我们需要包含==“问题 - 推论过程 - 答案”==三元组的训练数据。
    - **如何获取推论过程？** 这是最难的部分。
        - **人工标注：** 最直接但成本极高，因为撰写详细的推论过程非常耗时。
        - **语言模型生成：**
            1. 用一个语言模型（通常是能力较强的模型）对问题进行CoT推理，生成推论过程和答案。
            2. 由于模型可能出错，需要根据==最终答案的正确性==来筛选（如果答案正确，则假设推论过程也大概率是正确的）。
            3. 或者用一个==Verifier模型==来判断答案的正确性，然后保留验证为正确的推论过程。
- **超越最终答案验证：Tree-based Verification (树状验证)**
    - **问题：** 如果只看最终答案是否正确来判断推论过程的好坏，那么可能会出现“答案正确但推论过程错误”的情况。
    - **解决方案：** 结合方法二中的==树状搜索和Process Verifier==。
        1. 模型在解题时展开一个树状结构。
        2. Process Verifier验证每一步是否可能是正确的。
        3. 只继续展开那些“可能正确”的步骤。
        4. 如果最终得到正确答案，那么这条路径上的中间步骤也被认为可能是正确的。
        5. 将这些经过验证的、每一步都“看起来正确”的推论过程作为训练数据，来指导模型学习。
- **结合强化学习 (Reinforcement Learning) 思想：**
    - 模仿学习不一定局限于监督学习，也可以借鉴强化学习的思想，例如==DPO（Direct Preference Optimization）==。
    - 在树状结构中，我们知道哪些路径导致正确答案（奖励高），哪些导致错误答案（奖励低）。
    - 可以训练模型==增加正确路径的生成概率==，==减少错误路径的生成概率==。
- **允许“犯错并改正”的推论过程：**
    - **核心洞察：** 实际深度思考模型（如GPT-4o）的推论过程有时也会包含错误，但它们能够==自我修正==。
    - **传统模仿学习的不足：** 如果只用“完全正确”的推论过程训练模型，模型就只会“打顺风局”，一旦中间出错，它不知道如何修正，可能会一直“硬凹”下去。
    - **解决方案：** 在训练数据中==故意包含一些“错误但最终修正”的推论路径==。
        - **String of Search 论文：** 提出在树状结构中进行深度优先搜索，将包含错误步骤但最终得到正确答案的路径也纳入训练数据。
        - **Verbosity：** 为了让人类读者理解，可以在错误步骤和修正步骤之间插入Verifier的反馈（例如：“这个步骤错了，我们应该换一个解法。”）或者概括性的语句（“让我们一切重新来过。”）。
        - **Journey Learning 论文：** 与Shortcut Learning（只学习正确路径）相对，Journey Learning鼓励模型学习如何==从错误路径转回正确路径==，从而学会“逆转胜”。实验证明Journey Learning能提升模型表现。
    ```
    graph TD;
        A[模仿学习] --> B[核心: 教模型如何推理];
        B --> B1[需要“问题-推理过程-答案”数据];
        B --> C[推理过程来源];
        C --> C1[人工标注 (昂贵)];
        C --> C2[LLM生成 + 筛选];
        C2 --> C3[LLM生成 + Verifier验证];
        B --> D[训练策略];
        D --> D1[监督微调 (Supervised Fine-tuning)];
        D --> D2[结合树状验证 (Process Verifier)];
        D2 --> D3[允许错误并修正的路径];
        D3 --> D4[String of Search / Journey Learning];
        D --> D5[结合强化学习 (如DPO)];
        B --> E[知识蒸馏 (Knowledge Distillation)];
        E --> E1[强模型生成推理过程];
        E1 --> E2[弱模型学习其推理过程];
        E --> E3[DeepSeek-R1 论文验证其有效性];
    ```
- **知识蒸馏 (Knowledge Distillation)：**
    - **概念：** 如果已经存在一个具有深度思考能力的“老师”模型，可以直接让它生成推理过程和答案。然后，用这些由“老师”生成的“问题 - 推论过程 - 答案”数据来==微调（fine-tune）一个“学生”模型==。
    - **优点：** 这种方法可以==快速赋予一个模型推理能力==，并且比从头训练成本更低。
    - **应用：** SkyPilot T1 和 S1 都使用了这种方法来训练其模型的推理能力。
    - **DeepSeek-R1 论文验证：** DeepSeek-R1的论文中也展示了知识蒸馏的有效性。他们用DeepSeek-R1作为“老师”，训练了Qwen和Llama等模型，发现这些模型在数学和编程任务上的能力显著提升。
### 0.4.4 方法四：结果导向学习 (Reinforcement Learning - RL) - 需要微调参数
> 💡 **通俗概括：** 这种方法很“粗暴”，它不关心模型怎么想的，只关心最后答案对不对。只要答案对了就给奖励，错了就给惩罚，让模型自己去摸索出正确的思考路径。
- **概念：** ==Reinforcement Learning (RL，强化学习)==，==以最终结果为导向==来训练模型。
    - **核心：** 输入问题，模型进行推理并给出答案。我们==只看最终答案是否正确==。
    - **奖励机制：**
        - 如果答案正确，模型获得==正向奖励（positive reward）==。
        - 如果答案错误，模型获得==负向奖励（negative reward）==。
    - **不关心过程：** 在RL训练过程中，模型推论的==具体内容（推理过程说了什么）并不重要==，只要最终答案是对的，它就得到了奖励。
- **DeepSeek-R1-0 示例：**
    - **模型：** DeepSeek-R1-0 是一个纯粹通过RL训练出来的模型。
    - **基础模型：** 它以DeepSeek-V3-base（一个强大的基础模型）为起点。
    - **奖励设计：**
        - **主要奖励：** 模型回答问题的正确率。
        - **额外奖励：** ==格式奖励（format reward）==，例如要求模型生成 `think` 标记。这就是为什么经过RL训练后，模型能自动生成这些特定标记。
- **RL方法的有效性：**
    - DeepSeek-R1的论文展示了R1-0的强大效果。在AIME数学竞赛问题上（非常难），经过RL训练的R1-0可以逼近甚至与O系列模型（如GPT-4o）表现相当。
    - 更重要的是，R1-0通过RL训练后，==结合方法二的Majority Vote，可以进一步强化其能力==。这表明不同方法之间是**不冲突**的，可以叠加使用。
    ```
    graph TD;
        A[结果导向学习 (RL)] --> B[核心: 以最终结果为导向];
        B --> B1[只关心答案是否正确];
        B1 --> B2[推理过程内容不重要];
        A --> C[奖励机制];
        C --> C1[正确答案: Positive Reward];
        C1 --> C2[错误答案: Negative Reward];
        C --> C3[额外奖励: 特定格式 (如`think` token)];
        A --> D[示例: DeepSeek-R1-0];
        D --> D1[基础模型: DeepSeek-V3-base];
        D1 --> D2[RL训练];
        D2 --> D3[成果: 高正确率, 逼近顶尖模型];
        D3 --> D4[可与Majority Vote结合, 进一步强化];
        A --> E[“Aha Moment”];
        E --> E1[模型自行学习到错误修正能力];
        E1 --> E2[强化原有能力];
    ```
- **“Aha Moment”：RL自行学到的能力**
    - DeepSeek研究人员发现，经过RL训练的模型，能够==自行学到“发现并修正错误”的能力==，他们称之为“Aha Moment”。
    - 这意味着模型在推理过程中，会突然意识到之前可能错了，然后重新审视思路。
    - **深层原因：** RL并不是从零创造能力，而是==强化了基础模型（Foundation Model）原有的能力==。多项研究表明，像DeepSeek-V3-base这样的基础模型在RL训练前就已经具备了自我质疑、自我检查的行为（例如会说“Wait, I'm overthinking”或“Let's check if we made an error”）。RL只是放大了这些行为出现的频率。
- **RL的局限性：推论过程难以理解**
    - 尽管RL效果强大，但纯粹用RL训练的模型（如DeepSeek-R1-0）其推论过程往往==非常难以阅读==，==甚至会夹杂多种语言==。
    - **原因：** 因为在训练时==只关注结果，而没有对推论过程本身进行人工干预或监督==。模型为了达成目标（正确答案）可能会生成人类难以理解的“内部逻辑”。
## 0.5 第四讲：案例分析：DeepSeek-R1 的复杂打造过程
> 💡 **通俗概括：** DeepSeek-R1这个“思考大师”不是一蹴而就的，它是结合了前面讲的各种“法宝”，经过好几轮的“训练营”才最终炼成的，包括先让它自己瞎琢磨，再人工精修，最后才成了我们看到的样子。
很多关于DeepSeek-R系列的“农场文”只简单地说它“用RL训练就结束了”，但实际上，DeepSeek-R1的打造过程非常复杂，融合了前述的所有方法。
```
graph TD;
    A[DeepSeek-R1 复杂打造过程] --> B[第一阶段: 纯RL模型 R1-0];
    B --> B1[基础模型: DeepSeek-V3-base];
    B1 --> B2[RL训练 (正确率+格式奖励)];
    B2 --> B3[结果: DeepSeek-R1-0 (推理过程难读)];
    A --> C[第二阶段: 模仿学习 (Model A)];
    C --> C1[R1-0生成推理数据];
    C1 --> C2[大量人工修改/选择 (耗时)];
    C --> C3[FU-shot CoT (Supervised CoT) 生成数据];
    C3 --> C4[收集"thousands of examples"];
    C4 --> C5[Imitation Learning (微调 DeepSeek-V3)];
    C5 --> C6[结果: Model A (推理过程较可读)];
    A --> D[第三阶段: 再次RL (Model B)];
    D --> D1[Model A 进一步RL];
    D1 --> D2[奖励: 正确率 + 语言一致性];
    D2 --> D3[结果: Model B (语言一致性更好, 但正确率稍降)];
    A --> E[第四阶段: 大规模模仿学习 (Model C)];
    E --> E1[Model B 生成大量推理数据 (60万条)];
    E1 --> E2[DeepSeek-V3 Self-Output 数据 (20万条)];
    E2 --> E3[规则过滤糟糕推理过程 (多语言/过长/代码)];
    E3 --> E4[Imitation Learning (微调 DeepSeek-V3)];
    E4 --> E5[结果: Model C];
    A --> F[第五阶段: 最终RL (DeepSeek-R1)];
    F --> F1[Model C 再次RL];
    F1 --> F2[目标: 强化Safety和Helpfulness];
    F2 --> F3[结果: DeepSeek-R1 (最终产品)];
    A --> G[总结];
    G --> G1[结合多种方法 (CoT, RL, IL)];
    G1 --> G2[并非纯粹RL];
    G1 --> G3[TCS/MCTS仍在探索中];
```
- **阶段一：DeepSeek-R1-0 的诞生 (纯RL)**
    - 以 **DeepSeek-V3-base** 为基础模型。
    - 通过RL进行训练，==以问题答案的正确率和格式（生成 `think` token）为奖励==，打造出DeepSeek-R1-0。
    - **特点：** R1-0推理能力强，但其推理过程==人类难以阅读，且多语言混杂==。
- **阶段二：模仿学习，打造 Model A (优化可读性)**
    - **数据来源：**
        1. 由R1-0生成带有推理过程的训练数据，但这些数据需要==耗费大量人工进行修改和筛选==，以提高其可读性。
        2. 使用 **Supervised CoT** 方法（即方法一），通过复杂的Prompt强制另一个模型生成带有反射（reflection）和验证（verification）的详细回答。这些数据量较少，但有人工精选和介入。
    - **训练：** 将这些经过人工优化的数据用于**模仿学习**（微调DeepSeek-V3-base）。
    - **结果：** 得到 **Model A**，其推理过程比R1-0更易读。
- **阶段三：再次RL，打造 Model B (语言一致性)**
    - **训练：** Model A 进一步进行RL训练。
    - **奖励：** 除了正确率奖励外，增加了一个==“语言一致性”奖励==。如果模型在推理过程中都使用同一种语言（例如都用英文或都用中文），就会得到额外奖励。
    - **目的：** 强制模型在推理时保持语言的统一性，避免多语言混杂。
    - **结果：** 得到 **Model B**。虽然整体正确率可能略有下降（因为增加了限制），但其推理过程的可读性进一步提升。
- **阶段四：大规模模仿学习，打造 Model C (多任务泛化)**
    - **数据来源：**
        1. Model B 生成大量的带有推理过程的训练数据（约**60万笔**），这次涵盖了==各种不同的任务==（不再局限于数学和编程）。
        2. 同时，还加入了约**20万笔**来自DeepSeek-V3的**Self-Output**数据，以防止模型忘记基础能力。
        3. 对这些生成的推理数据进行==规则过滤==，剔除多语言、过长或包含不必要代码等“糟糕”的推理过程。
    - **训练：** 将这些大规模数据混合后，再次对DeepSeek-V3-base进行**模仿学习**。
    - **结果：** 得到 **Model C**。
- **阶段五：最终RL，打造 DeepSeek-R1 (安全与助益性)**
    - **训练：** Model C 进行==最后一次RL训练==。
    - **目标：** 这一阶段的RL主要目标是强化模型的==Safety（安全性）和Helpfulness（助益性）==能力。
    - **结果：** 最终得到用户实际使用的 **DeepSeek-R1** 模型。
### 0.5.1 总结与观察
- **多种方法融合：** DeepSeek-R1的打造过程清楚地表明，它并非单一方法（如纯RL）的产物，而是==巧妙地融合了CoT、模仿学习和强化学习等多种方法==。
- **RL与基础模型能力：** RL的成功==高度依赖于基础模型本身的能力==。对于像Qwen 32B这样较弱的模型，直接进行RL强化效果不佳，但通过知识蒸馏（模仿学习DeepSeek-R1）则能大幅提升其能力。
- **尚待研究：** DeepSeek的技术报告中提到，他们曾尝试Process Verifier和MCTS，但最终未能取得好的结果，这仍然是一个开放的研究问题。
- **DeepSeek-R1的推理过程特点：** 尽管经过多轮优化，DeepSeek-R1在推理过程中有时仍会产生==“奇奇怪怪”的输出==（例如括号不配对，语句不通顺），这表明在训练过程中，相比于推理过程的流畅度和逻辑严谨性，==模型最终答案的正确性仍然是主要关注点==，对过程的人工监督相对较少。
## 0.6 第五讲：挑战与未来发展：让模型“适度思考”
> 💡 **通俗概括：** 现在的“思考大师”有时太爱表现，简单问题也“长篇大论”，又费钱又费力。未来的目标是让它学会“看菜下碟”，该深思时深思，该秒回时秒回。
### 0.6.1 当前推理模型面临的挑战
- **成本与算力消耗：** 产生非常长的Reasoning过程==显然会消耗更多的计算资源和时间==，导致成本增加。
- **“无谓的推理”：** 当前的深度思考模型有时会进行==不必要的、冗长的推理==。即使一个简单的问题，它们也会“过度思考”，导致资源浪费。
- **核心期望：** 我们希望模型能够将它的“力量用在刀刃上”：==该Reasoning的时候才Reasoning，不该Reasoning的时候不要做无谓的Reasoning==。
### 0.6.2 DeepSeek-R1 的冗长推理示例
让我们回顾DeepSeek-R1解答 `123 × 456` 的完整过程，这个例子完美地诠释了“无谓的推理”：
- **初始计划：** 将456拆解为 `400+50+6`，然后逐项与123相乘，最后相加。
- **计算 `123 × 400`：**
    - 先算 `123 × 4 = 492`。
    - **中途验算：** 通过 `100×4 + 20×4 + 3×4` 验证 `492` 的正确性。
    - 得到 `49200`。
- **计算 `123 × 50`：**
    - 子计划：`123 × 5` 再加一个零。
    - 得到 `6150`。
- **计算 `123 × 6`：** 得到 `738`。
- **求和：** 将 `49200 + 6150 + 738` 相加，得到 `56088`。
- **多轮冗余验算：** 模型反复对 `56088` 进行验算，使用了多种方法：
    - **方法一：** 验算最后一步 `55350 + 738`。
    - **方法二：** 改变乘法顺序（例如 `123 × 6`， `123 × 50`， `123 × 400` 再相加）。
    - **方法三：** 估算 (`123` 约等于 `120`，`456` 约等于 `450`，相乘估算）。
    - **方法四：** 再次拆解 `123` (`100+20+3`)，然后分步相乘再相加。
    - **方法五：** “心算”直接得到 `56088`（此处尤其令人疑惑，既然能心算，为何前面大费周章？）。
    - **方法六：** 展开为九项乘法（`(100+20+3) × (400+50+6)`）。
- **观察：** 模型在第一次得到正确答案 `56088` 后，仍然进行了==大量重复且不必要的验算==，这极大地浪费了时间和算力。
### 0.6.3 未来发展方向
- **“刀刃上”的推理：** 下一堂课将深入探讨如何让模型能够==智能地选择何时进行深度推理，何时直接给出答案==，从而缩短不必要的推论过程，优化算力使用效率。
## 0.7 成为大师：对比总结
|   |   |   |   |   |
|---|---|---|---|---|
|**特征/方法**|**方法一：更好的CoT (无需微调)**|**方法二：直接给定工作流程 (无需微调)**|**方法三：模仿学习 (微调)**|**方法四：结果导向学习 (微调)**|
|**通俗概括**|老师教步骤，学生跟着做|让模型多试几次，总能蒙对|学生模仿老师思考过程，学其所思|只看结果，不问过程，对错有奖惩|
|**是否需要微调**|否|否|是|是|
|**核心机制**|通过Prompt引导模型生成解题步骤|通过多次采样/复杂搜索策略强制模型行为|通过“问题-推理过程-答案”数据监督学习|通过奖励信号（答案正确性）强化模型行为|
|**数据需求**|少量Few-shot示例或Zero-shot指令|无需特定数据，依赖模型生成的多样性|包含完整推理过程的标注数据（或生成后筛选）|仅需问题-正确答案对|
|**推理过程**|相对直接，受Prompt引导|通过外部控制（如多数投票、Beam Search）选择最佳路径|模型学得“老师”的推理过程|模型自行探索，过程可能难读或混乱|
|**优势**|简单易用，对强模型效果好|可大幅提升弱模型性能，尤其适用于答案单一问题|能够学习复杂的推理结构和纠错能力|效果强大，可自行发现“Aha Moment”，潜力大|
|**劣势**|对弱模型效果不佳，Prompt设计复杂|需要大量推理次数，效率较低，选择答案策略复杂|数据获取困难，推理过程质量取决于数据质量|推理过程难读，对基础模型能力依赖高，可能过拟合|
|**典型应用**|Zero-shot CoT for复杂问题|Majority Vote, Beam Search, LLM as a Monkey|Knowledge Distillation, Supervised Fine-tuning|DeepSeek-R1-0, RLHF for reasoning|
|**DeepSeek-R1 应用**|在第二阶段用于生成部分数据|(间接应用，通过其强化RL模型)|在第二、第四阶段的核心方法|在第一、第三、第五阶段的核心方法|
**大师总结：**
从“小白”到“大师”，你会发现大语言模型的“深度思考”并非玄学。它本质上是**在测试阶段投入更多算力**（Test-Time Compute），通过延长模型的==生成“长度”==来弥补其“深度”的不足。这包括了从简单的Prompt工程（CoT），到复杂的外部算法控制（如Beam Search），再到需要模型自身参数学习的微调方法（模仿学习和强化学习）。
最先进的模型（如DeepSeek-R1）并非单一方法的产物，而是**多种方法的“混血儿”**。它们先通过强化学习让模型拥有强大的“摸索”能力，再通过人工干预和模仿学习来“雕琢”其推理过程的可读性，最后再进行一轮强化学习来优化其他特性。
然而，当前的“深度思考”依然面临挑战，==最核心的是效率问题==。模型常常“过度思考”，导致资源浪费。未来的研究方向将是如何让模型学会“节约”，在真正需要的时候才进行深度推理，这将是下一个重要的突破点。