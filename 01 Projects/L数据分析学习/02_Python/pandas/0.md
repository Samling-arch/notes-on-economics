
# 1 Pandas 核心知识点学习笔记（上）

你好！欢迎来到 Pandas 的世界。把它想象成一个超强的乐高工具箱，专门用来拼搭和整理各种数据积木。这份笔记会带你认识工具箱里最重要的几件工具，并学会如何使用它们。

首先，按照你的要求，这里是 Obsidian 中 Mermaid 语法的标准示范，这个图表展示了一个简单的决策流程：

```mermaid
graph TD;
    A[开始] --> B{是或否?};
    B -- 是 --> C[操作一];
    B -- 否 --> D[操作二];
    C --> E[结束];
    D --> E;
```
这个语法就像在给电脑画画下指令，`graph TD` 表示我们要画一个从上到下（Top Down）的流程图，`-->` 表示箭头，`[]` 表示方块，`{}` 表示菱形判断。Obsidian看到 ` ```mermaid ` 就知道，这里要施展“画图魔法”了。

---

## 1.1 零、Pandas 的三大核心基石

在正式开始数据分析的流程前，我们必须先认识 Pandas 工具箱里最核心的三种“积木块”。

> **一句话精髓**：Pandas 的世界由三种基本元素构成：`Index`（目录）、`Series`（一列数据）和 `DataFrame`（一张完整的表格）。

### 1.1.1 📚 知识框架 (Mermaid)

```mermaid
graph TD
    subgraph Pandas 核心数据结构
        A[DataFrame (二维数据/表格)]
        B[Series (一维数据/一列)]
        C[Index (索引/目录)]

        A -- "由多个 Series 组成" --> B
        A -- "拥有行索引(index)和列索引(columns)" --> C
        B -- "拥有一个索引(index)" --> C
    end
```

### 1.1.2 🧐 细节深度剖析

#### 1.1.2.1 `Series`：一维数据

- **它是什么？** `Series` 就像是表格中的**一列**数据。想象一下班级的花名册，其中“姓名”那一列，或者“数学成绩”那一列，就是一个 `Series`。它由两部分组成：数据本身和与之对应的索引（可以理解为行号或标签）。
- **给小朋友讲**：`Series` 就像一串糖葫芦，每个山楂就是一个数据，而串起它们的竹签就是索引，让你能准确找到第几个山楂。
- **代码示例**：
  ```python
  import pandas as pd

  # 创建一个 Series，代表几个学生的年龄
  ages = pd.Series([10, 11, 10, 12], index=['小明', '小红', '小刚', '小丽'])
  print(ages)
  ```
  输出：
  ```
  小明    10
  小红    11
  小刚    10
  小丽    12
  dtype: int64
  ```

#### 1.1.2.2 `DataFrame`：二维数据

- **它是什么？** `DataFrame` 是 Pandas 的**核心**，它就是一个完整的**电子表格**。它有行也有列，就像我们熟悉的 Excel 表格一样。你可以把它看作是多个 `Series` 共用同一个行索引组成的“大家庭”。
- **给小朋友讲**：`DataFrame` 就像一个班级的点名册，有“姓名”、“年龄”、“分数”好几列（每一列都是一个 `Series`），还有每一行的学生名字作为目录（行索引）。
- **代码示例**：
  ```python
  import pandas as pd

  # 创建一个 DataFrame
  data = {'姓名': ['小明', '小红', '小刚', '小丽'],
          '年龄': [10, 11, 10, 12],
          '分数': [95, 98, 89, 92]}
  df = pd.DataFrame(data)
  print(df)
  ```
  输出：
  ```
     姓名  年龄  分数
  0  小明  10  95
  1  小红  11  98
  2  小刚  10  89
  3  小丽  12  92
  ```

#### 1.1.2.3 `Index`：索引服务

- **它是什么？** `Index` 是为 `Series` 和 `DataFrame` 提供**地址服务**的。它就像书的**目录**或者字典的**首字母索引**，帮助我们快速、准确地找到想要的数据。`DataFrame` 有两个索引：
    - `index`：行索引，通常是每一行的标签。
    - `columns`：列索引，就是我们常说的“表头”或“列名”。
- **给小朋友讲**：`Index` 就是你家小区的门牌号（行索引）和街道名（列索引）。有了它们，快递员（我们）就能准确地把包裹（数据）送到你家。
- **特别注意**：视频中提到了 `MultiIndex`（多级索引），这就像是更复杂的地址，比如“XX省-XX市-XX区”，可以进行更精细的数据定位，我们后面会深入学习。

---

## 1.2 一、数据分析第一步：数据加载

这是我们数据分析旅程的起点。数据通常存放在外部文件里，我们需要把它们“请”到我们的程序中，变成 Pandas 能识别的 `DataFrame`。

> **一句话精髓**：教会 Pandas 如何从各种地方（如 CSV 文件、Excel、数据库）读取数据，并把它们变成我们能操作的智能表格（`DataFrame`）。

### 1.2.1 📚 知识框架 (Mermaid)

```mermaid
graph TD
    subgraph 数据加载: 创建 DataFrame
        A[从文件读取]
        B[从数据库读取]
        C[直接用代码创建]
    end
    A --> A1[pd.read_csv()]
    A --> A2[pd.read_excel()]
    A -- "其他格式..." --> A3[...]
    B --> B1[pd.read_sql()]
```

### 1.2.2 🧐 细节深度剖析

#### 1.2.2.1 从 CSV 文件加载：`pd.read_csv()`

CSV (Comma-Separated Values) 是一种非常常见的纯文本数据格式，用逗号来分隔数据。

- **给小朋友讲**：`read_csv` 就像一个聪明的阅读机器人，你给它一个用逗号隔开所有信息的记事本文件，它就能读懂并帮你整理成一张漂亮的表格。

- **核心参数详解**（小白必知）：

    - `filepath_or_buffer`: **文件路径**。你要读的 CSV 文件在哪里。
      - `例子`: `"data/students.csv"`
    - `sep` 或 `delimiter`: **分隔符**。告诉机器人数据是用什么符号隔开的。默认是逗号 `,`。如果文件用的是分号 `;` 或制表符 `\t`，就需要指定。
      - `例子`: `sep=';'`
    - `header`: **表头在哪一行**。默认是第 `0` 行（也就是第一行）。如果表头在第三行，就写 `header=2`。
      - `例子`: `header=2`
    - `encoding`: **编码格式**。防止中文乱码。如果打开文件看到乱码，很可能需要设置这个，常见的值有 `'utf-8'` 或 `'gbk'`。
      - `例子`: `encoding='gbk'`
    - `quotechar`: **引用字符**。如果你的文本数据被特殊的引号（比如 `"`）包起来了，这个参数可以告诉 Pandas 自动帮你剥掉这些引号。
      - `例子`: `quotechar='"'`
    - `usecols`: **只读取指定的列**。当文件列非常多，但你只需要其中几列时，用这个可以节省内存和时间。
      - `例子`: `usecols=['姓名', '分数']`
    - `index_col`: **指定哪一列作为行索引**。把某一列（如“学号”）变成我们前面说的行“目录”。
      - `例子`: `index_col='学号'`
    - `nrows`: **只读取前 N 行**。用于快速预览一个超大文件。
      - `例子`: `nrows=100`
    - `skiprows`: **跳过指定的行**。比如文件开头有几行说明文字，可以用这个跳过。
      - `例子`: `skiprows=[0, 2]` (跳过第1行和第3行)
    - `na_values`: **哪些值被视为空值**。告诉 Pandas，除了空白，像 "N/A", "missing", "未知" 这些也算是“缺失数据”。
      - `例子`: `na_values=['N/A', '未知']`
    - `iterator=True` 与 `chunksize`: **分块读取大文件**。当文件太大，内存一次性装不下时，可以开启“迭代器模式” (`iterator=True`)，并指定每次读取的“块”的大小 (`chunksize`)，像蚂蚁搬家一样，一块一块地处理。
      - `例子`: `iterator=True, chunksize=10000`
    - `engine`: **更换读取引擎**。Pandas 默认用 `c` 引擎（底层是 NumPy）读取，速度已经很快。但对于某些情况，更换成 `pyarrow` 引擎可能会更快，并且更省内存。需要先安装 `pyarrow` 库。
      - `例子`: `engine='pyarrow'`

- **代码示例**：
  ```python
  # 假设我们有一个 'students.csv' 文件
  # 内容:
  # 姓名,年龄,城市
  # 小明,10,北京
  # 小红,11,上海
  
  df = pd.read_csv('students.csv', index_col='姓名')
  print(df)
  ```

#### 1.2.2.2 从 Excel 文件加载：`pd.read_excel()`

这个函数和 `read_csv` 非常相似，但专门用于读取 `.xls` 或 `.xlsx` 格式的 Excel 文件。

- **给小朋友讲**：这个机器人更厉害，它能直接打开 Excel 表格，而不是只能读记事本。

- **与 `read_csv` 的主要区别**：
    - `sheet_name`: **指定读取哪个工作表**。一个 Excel 文件里可以有好几张表，你需要告诉它读哪一张。可以给表名（字符串）或者表的序号（从 `0` 开始）。
      - `例子`: `sheet_name='Sheet1'` 或 `sheet_name=0`
    - **没有迭代器模式**：`read_excel` 不支持像 `read_csv` 那样的 `iterator` 和 `chunksize` 参数来分块读取。

- **代码示例**：
  ```python
  # 读取 'grades.xlsx' 文件中的 '期末成绩' 这张表
  df_grades = pd.read_excel('grades.xlsx', sheet_name='期末成绩')
  print(df_grades)
  ```

#### 1.2.2.3 从数据库加载：`pd.read_sql()`

直接连接数据库服务器（如 MySQL），执行 SQL 查询语句，并将结果加载为 `DataFrame`。

- **给小朋友讲**：这回机器人变成了一个特工，能直接潜入存放海量信息的“数据大本营”（数据库），按照你的指令（SQL 语句）取出情报，并整理成表格。

- **核心步骤**：
    1. **安装驱动**：首先要安装能让 Python 和数据库“对话”的工具。对于 MySQL，视频中提到了 `PyMySQL` 和 `SQLAlchemy`。
       ```bash
       pip install pymysql sqlalchemy cryptography
       ```
    2. **创建连接引擎**：使用 `SQLAlchemy` 创建一个到数据库的“连接通道”。
       ```python
       from sqlalchemy import create_engine
       # 格式: '数据库类型+驱动://用户名:密码@主机地址:端口/数据库名'
       engine = create_engine('mysql+pymysql://user:password@localhost:3306/mydatabase')
       ```
    3. **读取数据**：使用 `pd.read_sql()` 函数。
       - `sql`: **SQL 查询语句或表名**。你可以写一个完整的 `SELECT` 语句，也可以直接给一个表名。
       - `con`: **连接引擎**。就是我们上一步创建的 `engine`。
       - `index_col`: 同样可以指定某一列作为行索引。

- **对比总结：`read_sql` 的两种用法**
    | 函数/用法 | 说明 | 适用场景 |
    | :--- | :--- | :--- |
    | `read_sql(sql_query, con)` | `sql_query` 是一个 `SELECT` 语句 | 最常用，最灵活，可以进行复杂的查询、筛选和连接。 |
    | `read_sql(table_name, con)` | `table_name` 是一个数据库表的名称 | 简单快捷，相当于 `SELECT * FROM table_name`。 |

- **代码示例**：
  ```python
  from sqlalchemy import create_engine
  import pandas as pd

  # 1. 创建引擎 (请替换为你的数据库信息)
  # engine = create_engine('mysql+pymysql://user:password@host:port/database')

  # 2. 编写 SQL 语句
  sql_query = "SELECT id, name, age FROM users WHERE age > 18;"
  
  # 3. 读取数据 (假设 engine 已创建好)
  # df_users = pd.read_sql(sql_query, engine, index_col='id')
  # print(df_users)
  ```

---

## 1.3 二、数据准备 Part 1：基础操作与结构调整

数据加载进来后，它是一个 `DataFrame` 对象。现在我们要学习如何与这个“智能表格”互动，比如查看信息、选取特定的行列、增删改查等。

> **一句话精髓**：学会像操作棋盘一样，精准地查看、拾取、移动和调整我们的数据棋子（行、列、单元格）。

### 1.3.1 📚 知识框架 (Mermaid)

```mermaid
graph TD
    subgraph DataFrame 基础操作
        A[查看与概览] --> A1[df.info()]
        A --> A2[df.head() / df.tail()]
        A --> A3[df.sample()]
        
        B[数据选取(取子集)] --> B1["列操作<br>df['列名'] 或 df.列名"]
        B --> B2["行操作<br>df.loc[] / df.iloc[]"]
        B --> B3["单元格操作<br>df.at[] / df.iat[]"]
        B --> B4["条件筛选<br>df.query() / 布尔索引"]

        C[数据修改与结构调整] --> C1[删除: df.drop()]
        C --> C2[重命名索引: df.rename()]
        C --> C3["重置/设置索引<br>df.reset_index() / df.set_index()"]
        C --> C4[调整顺序: df.reindex()]
        C --> C5[索引排序: df.sort_index()]
    end
```
### 1.3.2 🧐 细节深度剖析

#### 1.3.2.1 查看与概览 (Data Inspection)

在你对数据做任何操作之前，先“体检”一下。

- `df.info()`: **显示 DataFrame 的摘要信息**。这是你拿到数据后**第一个**应该使用的命令！它会告诉你：
    - 总共有多少行，多少列。
    -每一列的名称、非空值的数量、数据类型（比如是数字 `int64`、文字 `object` 还是日期 `datetime64`）。
    - 内存占用情况。
- `df.head(n)`: **查看前 n 行**，默认 `n=5`。快速看看数据长什么样。
- `df.tail(n)`: **查看后 n 行**，默认 `n=5`。
- `df.sample(n)`: **随机抽取 n 行**。可以帮你了解数据的随机样本情况。
  - `frac`: 也可以按比例抽样，`df.sample(frac=0.1)` 表示随机抽取 10% 的数据。

#### 1.3.2.2 数据选取 (Slicing and Dicing)

这是 Pandas 最强大、最常用的功能之一。

- **操作列**:
    - `df['列名']`: 最常用、最稳妥的方式。返回一个 `Series`。
    - `df.列名`: 点语法，更简洁。但**有条件**：列名必须是有效的 Python 标识符（不能有空格、不能是数字开头、不能是 Python 关键字等）。
- **操作行 (`loc` vs `iloc`)**: 这是新手的第一个难点，但非常重要！
    | 特性 | `df.loc[]` (Location) | `df.iloc[]` (Integer Location) |
    | :--- | :--- | :--- |
    | **索引方式** | 基于**标签 (Label)** | 基于**整数位置 (Integer Position)** |
    | **给小朋友讲** | 像在图书馆按**书名**找书。 | 像在书架上找**第 5 本**书。 |
    | **用法** | `df.loc['行标签', '列标签']` | `df.iloc[行号, 列号]` |
    | **切片** | `df.loc['A':'C']` **包含**结束位置 C | `df.iloc[0:3]` **不包含**结束位置 3 |
    | **示例** | `df.loc['小明']` | `df.iloc[0]` |

- **操作单元格 (`at` vs `iat`)**: `loc` 和 `iloc` 的“快速版”，专门用于获取或设置**单个值**，性能更好。
    | 特性 | `df.at[]` | `df.iat[]` |
    | :--- | :--- | :--- |
    | **索引方式** | 基于**标签 (Label)** | 基于**整数位置 (Integer Position)** |
    | **用法** | `df.at['行标签', '列标签']` | `df.iat[行号, 列号]` |

- **条件筛选**:
    - **布尔索引 (Boolean Indexing)**:
        1. 创建一个条件，得到一个布尔 `Series` (一串 `True`/`False`)。
        2. 把这个布尔 `Series` 扔进 `[]` 里，Pandas 会只保留 `True` 对应的行。
        - `例子`: `df[df['年龄'] > 10]`
    - **`df.query()`**: 更接近自然语言的筛选方式，把条件写成字符串。
        - `例子`: `df.query('年龄 > 10 and 城市 == "北京"')`

#### 1.3.2.3 结构调整 (Restructuring)

- `df.drop()`: **删除行或列**。
    - `df.drop('行标签')` 或 `df.drop(['行1', '行2'])`: 删除行。
    - `df.drop('列名', axis=1)`: 删除列（`axis=1` 表示沿列的方向操作）。
    - `inplace=True`: **就地修改**。默认 `drop` 会返回一个删除了数据的新 `DataFrame`，原 `df` 不变。设置 `inplace=True` 后，会直接在原 `df` 上删除，不返回任何东西。
- `df.rename()`: **重命名行/列索引**。通常使用字典来指定新旧名称的对应关系。
    - `例子`: `df.rename(columns={'年龄': 'Age'}, index={0: 'Zero'})`
- `df.reset_index()`: **重置索引**。将当前的行索引“降级”成一个普通的列，并使用默认的 `0, 1, 2...` 作为新的行索引。
    - `drop=True`: 如果不希望原来的索引变成新的一列，而是直接丢弃，就设置它。
- `df.set_index('列名')`: **设置索引**。将某一列“升级”为新的行索引。
- `df.reindex()`: **按新索引重新排序/对齐数据**。可以用来调整行或列的顺序。
    - `例子`: `df.reindex(columns=['分数', '年龄', '姓名'])`
- `df.sort_index()`: **按索引排序**。
    - `axis=0` (默认) 按行索引排序。
    - `axis=1` 按列索引排序。
    - `ascending=False`: 改为降序排列。

---
## 1.4 三、数据准备 Part 2：数据清洗与重塑

我们拿到的原始数据往往是不完美的，可能有缺失、有重复、有错误。这一步就是要把这些“脏”数据处理干净。同时，有时需要将多个表格拼接或合并起来，以进行更全面的分析。

> **一句话精髓**：把杂乱无章的原始数据，通过拼接、合并、清洗，变成干净、整洁、随时可以分析的“精装版”数据。

### 1.4.1 📚 知识框架 (Mermaid)

```mermaid
graph TD
    subgraph 数据准备 (Data Wrangling)
        A[数据重塑] --> A1[纵向拼接: pd.concat()]
        A --> A2[横向合并: pd.merge()]
        
        B[数据清洗] --> B1[处理缺失值 (Missing)]
        B --> B2[处理重复值 (Duplicate)]
        B --> B3[处理异常值 (Outlier)]

    end

    B1 --> B1a["甄别: isna() / notna()"]
    B1 --> B1b["删除: dropna()"]
    B1 --> B1c["填充: fillna() / interpolate()"]

    B2 --> B2a["甄别: duplicated()"]
    B2 --> B2b["删除: drop_duplicates()"]
    B2 --> B2c["统计: nunique() / value_counts()"]

    B3 --> B3a["甄别: 数值判定法 / Z-score法"]
    B3 --> B3b["处理: drop() / replace()"]
```

### 1.4.2 🧐 细节深度剖析

#### 1.4.2.1 数据重塑 (Reshaping)

##### 1.4.2.1.1 `pd.concat()`：拼接

- **它是做什么的？** 像**叠积木**一样，把多个结构相似的 `DataFrame` 沿着一个轴线拼接起来。
- **给小朋友讲**：你有好几天的日记，每篇日记都是一张小纸条（`DataFrame`），`concat` 就是用胶水把这些纸条从上到下粘成一张长长的纸。
- **核心参数**：
    - `objs`: 一个列表或元组，包含所有要拼接的 `DataFrame`。
    - `axis=0` (默认): **纵向拼接**（按行），要求列对齐。像叠罗汉。
    - `axis=1`: **横向拼接**（按列），要求行索引对齐。像手拉手排成一排。
    - `ignore_index=True`: 忽略原来各个 `DataFrame` 的索引，生成全新的 `0, 1, 2...` 连续索引。在纵向拼接时非常常用！
- **与 SQL 的对比**: `axis=0` 的纵向拼接，功能上类似于 SQL 中的 `UNION ALL`。
- **代码示例**:
  ```python
  df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})
  df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})
  result = pd.concat([df1, df2], ignore_index=True)
  print(result)
  ```

##### 1.4.2.1.2 `pd.merge()`：合并

- **它是做什么的？** 类似 SQL 的 `JOIN` 操作，根据一个或多个**共同的键**，将两个 `DataFrame` 的信息合并到一起。主要用于连接**事实表**和**维度表**。
- **给小朋友讲**：你有一张“成绩单”（事实表），上面有“学生ID”和“分数”。你还有一张“学生信息表”（维度表），上面有“学生ID”、“姓名”和“班级”。`merge` 就是通过共同的“学生ID”，把两张表的信息合并，让你知道每个分数具体是哪个班的哪个同学的。
- **核心参数**：
    - `left`, `right`: 左表和右表。
    - `how`: **连接方式**。
        - `'inner'` (内连接，默认): 只保留两张表中键都能匹配上的行。
        - `'outer'` (全外连接): 保留两张表所有的行，匹配不上的地方用空值填充。
        - `'left'` (左外连接): 保留左表所有的行，右表匹配不上的用空值填充。
        - `'right'` (右外连接): 保留右表所有的行，左表匹配不上的用空值填充。
    - `on`: **连接键**。如果左右表用来连接的列名相同，就用这个参数。
    - `left_on`, `right_on`: 如果左右表用来连接的列名**不同**，需分别指定。
    - `left_index=True`, `right_index=True`: 当连接键是表的**索引**时，使用这两个参数。

- **代码示例**:
  ```python
  left = pd.DataFrame({'key': ['K0', 'K1'], 'A': ['A0', 'A1']})
  right = pd.DataFrame({'key': ['K0', 'K1'], 'B': ['B0', 'B1']})
  result = pd.merge(left, right, on='key')
  print(result)
  ```

#### 1.4.2.2 数据清洗 (Data Cleaning)

##### 1.4.2.2.1 处理缺失值 (Missing Values)

- **甄别**:
    - `df.isna()` 或 `df.isnull()`: 返回一个布尔型 `DataFrame`，缺失值的位置为 `True`。
    - `df.notna()` 或 `df.notnull()`: 与上面相反，非缺失值的位置为 `True`。
- **处理方式**:
    1. **删除 `df.dropna()`**:
       - `axis=0` (默认): 删除任何**包含**缺失值的**行**。
       - `axis=1`: 删除任何**包含**缺失值的**列**。
    2. **填充 `df.fillna()`**:
       - `df.fillna(0)`: 用一个具体的值（如 `0`）填充。
       - `method='ffill'` (forward fill): 用**上一个**有效值向下填充。
       - `method='bfill'` (backward fill): 用**下一个**有效值向上填充。
    3. **插值 `df.interpolate()`**:
       - 更高级的填充，通过数学算法（如线性插值）计算并填充缺失值。适用于有趋势的数值数据。

##### 1.4.2.2.2 处理重复值 (Duplicate Values)

- **甄别 `df.duplicated()`**:
    - 返回一个布尔 `Series`，标记出哪些是**重复行**。默认除了第一次出现的，后面相同的都标记为 `True`。
    - `subset=['列1', '列2']`: 指定根据哪些列来判断是否重复。
- **删除 `df.drop_duplicates()`**:
    - 直接删除重复行。
    - `subset`: 同样可以指定判断重复的列。
    - `keep`:
        - `'first'` (默认): 保留第一次出现的，删除后面的。
        - `'last'`: 保留最后一次出现的，删除前面的。
        - `False`: 删除所有重复的行，一个都不留。
- **相关统计**:
    - `s.nunique()`: 计算一个 `Series` 中**唯一值**的个数。
    - `s.value_counts()`: 统计一个 `Series` 中**每个值出现的次数**（频率），并按高到低排序。非常有用，但视频中提到性能可能不佳，因为要计算和排序。
    - `s.unique()`: 返回一个 `Series` 中所有**唯一值**组成的数组。

##### 1.4.2.2.3 处理异常值 (Outliers / Anomalies)

异常值是那些“离群”的、不合常理的数据点。处理的关键在于**如何甄别**。

- **给小朋友讲**：班里同学身高都在 1.2米 到 1.4米 之间，突然有个数据是 5 米，这个 5 米就是异常值。

- **甄别方法**:

    1.  **数值判定法 (箱线图法 / IQR法)**:
        - **核心思想**: 定义一个“正常范围”，超出这个范围的就是异常值。
        - **步骤**:
            1. 计算四分位数：`Q1` (第25百分位数) 和 `Q3` (第75百分位数)。
            2. 计算四分位距：$IQR = Q3 - Q1$。这是数据中间 50% 的范围。
            3. 定义上下限：
               - 下限 (Lower Bound): $$L = Q1 - 1.5 \times IQR$$
               - 上限 (Upper Bound): $$U = Q3 + 1.5 \times IQR$$
            4. 任何小于下限或大于上限的数据点，被认为是**离群点**。

    2.  **Z-score 判定法 (3σ 法则)**:
        - **核心思想**: 如果一个数据点距离平均值太远（通常是超过3个标准差），它就是异常的。这个方法**假设数据大致服从正态分布**（钟形曲线）。
        - **直观解释 (68-95-99.7 法则)**:
            - 约 $68\%$ 的数据落在 `均值 ± 1个标准差` 内。
            - 约 $95\%$ 的数据落在 `均值 ± 2个标准差` 内。
            - 约 $99.7\%$ 的数据落在 `均值 ± 3个标准差` 内。
            - 所以，落在 `±3个标准差` 之外的数据是极小概率事件（仅 $0.3\%$），我们有理由怀疑它是异常的。
        - **Z-score 公式与推导**:
            - Z-score 的定义是：一个值偏离均值的程度，以标准差为单位。
            - **公式**: $$ Z = \frac{x - \mu}{\sigma} $$
                - $x$: 单个数据点。
                - $\mu$ (mu): 数据的平均值 (mean)。
                - $\sigma$ (sigma): 数据的标准差 (standard deviation)。
            - **推导**: 这不是一个需要“推导”的公式，而是一个**定义**。它将原始数据进行了**标准化**，使得所有数据都转换到一个以 `0` 为均值、`1` 为标准差的新尺度上。这样我们就可以用一个统一的标准（比如 Z-score 的绝对值是否大于 `3`）来判断任何数据集的异常值。
        - **判定**: 计算每个数据点的 Z-score，如果 $|Z| > 3$，则认为是异常值。

- **处理方法**:
    1. **删除**: 找到异常值所在的行，用 `df.drop()` 删除。
    2. **替换 `s.replace()`**: 用一个新值（比如均值、中位数）替换掉旧的异常值。
       - `regex=True`: 开启正则表达式替换，可以进行更复杂的模式匹配和替换。

---

**总结**: 这份笔记涵盖了视频“Pandas 核心知识点总结（上）”的全部内容。我们从 Pandas 的三大基石 `Series`, `DataFrame`, `Index` 出发，系统学习了数据分析流程中的**数据加载**和**数据准备**两大环节。你已经掌握了如何读取各种来源的数据，如何对 `DataFrame` 进行基础的查看、选取和结构调整，以及如何进行数据重塑（拼接、合并）和数据清洗（处理缺失、重复、异常值）。

接下来，你需要多多练习，把这些知识变成你自己的技能！希望这份详尽的笔记能成为你 Pandas 学习路上的得力助手。